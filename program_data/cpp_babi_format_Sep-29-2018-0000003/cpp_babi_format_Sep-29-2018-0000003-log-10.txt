Namespace(cuda=True, directory='program_data/cpp_babi_format_Sep-29-2018-0000003', is_training_ggnn=True, log_path='program_data/cpp_babi_format_Sep-29-2018-0000003/logs', lr=0.01, manualSeed=0, model_path='program_data/cpp_babi_format_Sep-29-2018-0000003/cpp_babi_format_Sep-29-2018-0000003-10.cpkl', n_classes=10, n_hidden=50, n_steps=5, niter=150, size_vocabulary=170, state_dim=5, test_batch_size=32, testing=False, train_batch_size=32, training=True, training_percentage=1.0, verbal=True, workers=0)
Random Seed:  0
  0% 0/10 [00:00<?, ?it/s] 10% 1/10 [00:00<00:01,  7.41it/s] 20% 2/10 [00:00<00:01,  6.30it/s] 30% 3/10 [00:00<00:01,  6.53it/s] 40% 4/10 [00:00<00:00,  6.40it/s] 50% 5/10 [00:00<00:00,  5.99it/s] 60% 6/10 [00:00<00:00,  6.37it/s] 70% 7/10 [00:01<00:00,  5.85it/s] 80% 8/10 [00:01<00:00,  5.10it/s] 90% 9/10 [00:01<00:00,  5.30it/s]100% 10/10 [00:01<00:00,  5.11it/s]
Number of all training data : 3330
Max node id : 169
  0% 0/10 [00:00<?, ?it/s] 20% 2/10 [00:00<00:00, 13.44it/s] 40% 4/10 [00:00<00:00, 11.69it/s] 60% 6/10 [00:00<00:00, 12.16it/s] 80% 8/10 [00:00<00:00, 12.16it/s]100% 10/10 [00:00<00:00, 10.25it/s]
Number of all testing data : 1670
Max node id : 164
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator stack because torch.onnx.symbolic.stack does not exist
  .format(op_name, op_name))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %98 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %161 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %224 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %287 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %350 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
[0/150][0/105] Loss: 2.3256
[0/150][11/105] Loss: 2.2928
[0/150][22/105] Loss: 2.3210
[0/150][33/105] Loss: 2.1520
[0/150][44/105] Loss: 2.0171
[0/150][55/105] Loss: 1.9462
[0/150][66/105] Loss: 1.9742
[0/150][77/105] Loss: 1.8323
[0/150][88/105] Loss: 1.9379
[0/150][99/105] Loss: 1.8605
Test set: Average loss: 0.0598, Accuracy: 971/1670 (58%)
[1/150][0/105] Loss: 1.9113
[1/150][11/105] Loss: 1.7948
[1/150][22/105] Loss: 1.8696
[1/150][33/105] Loss: 1.9775
[1/150][44/105] Loss: 1.7753
[1/150][55/105] Loss: 1.7549
[1/150][66/105] Loss: 1.7193
[1/150][77/105] Loss: 1.7560
[1/150][88/105] Loss: 1.8048
[1/150][99/105] Loss: 1.8822
Test set: Average loss: 0.0551, Accuracy: 1248/1670 (74%)
[2/150][0/105] Loss: 1.6688
[2/150][11/105] Loss: 1.7233
[2/150][22/105] Loss: 1.7794
[2/150][33/105] Loss: 1.7442
[2/150][44/105] Loss: 1.6661
[2/150][55/105] Loss: 1.6191
[2/150][66/105] Loss: 1.6755
[2/150][77/105] Loss: 1.6940
[2/150][88/105] Loss: 1.6994
[2/150][99/105] Loss: 1.6443
Test set: Average loss: 0.0539, Accuracy: 1295/1670 (77%)
[3/150][0/105] Loss: 1.6853
[3/150][11/105] Loss: 1.6639
[3/150][22/105] Loss: 1.7066
[3/150][33/105] Loss: 1.6995
[3/150][44/105] Loss: 1.6061
[3/150][55/105] Loss: 1.7054
[3/150][66/105] Loss: 1.6905
[3/150][77/105] Loss: 1.5993
[3/150][88/105] Loss: 1.6208
[3/150][99/105] Loss: 1.6698
Test set: Average loss: 0.0528, Accuracy: 1353/1670 (81%)
[4/150][0/105] Loss: 1.6920
[4/150][11/105] Loss: 1.5995
[4/150][22/105] Loss: 1.7100
[4/150][33/105] Loss: 1.6253
[4/150][44/105] Loss: 1.6192
[4/150][55/105] Loss: 1.6543
[4/150][66/105] Loss: 1.5994
[4/150][77/105] Loss: 1.6199
[4/150][88/105] Loss: 1.5831
[4/150][99/105] Loss: 1.7597
Test set: Average loss: 0.0532, Accuracy: 1330/1670 (79%)
[5/150][0/105] Loss: 1.6793
[5/150][11/105] Loss: 1.7764
[5/150][22/105] Loss: 1.6209
[5/150][33/105] Loss: 1.5694
[5/150][44/105] Loss: 1.5919
[5/150][55/105] Loss: 1.6647
[5/150][66/105] Loss: 1.6432
[5/150][77/105] Loss: 1.6504
[5/150][88/105] Loss: 1.6145
[5/150][99/105] Loss: 1.5305
Test set: Average loss: 0.0524, Accuracy: 1361/1670 (81%)
[6/150][0/105] Loss: 1.6589
[6/150][11/105] Loss: 1.6157
[6/150][22/105] Loss: 1.6082
[6/150][33/105] Loss: 1.6384
[6/150][44/105] Loss: 1.5589
[6/150][55/105] Loss: 1.5112
[6/150][66/105] Loss: 1.6139
[6/150][77/105] Loss: 1.6695
[6/150][88/105] Loss: 1.6445
[6/150][99/105] Loss: 1.5797
Test set: Average loss: 0.0518, Accuracy: 1387/1670 (83%)
[7/150][0/105] Loss: 1.5081
[7/150][11/105] Loss: 1.6009
[7/150][22/105] Loss: 1.5558
[7/150][33/105] Loss: 1.5840
[7/150][44/105] Loss: 1.6313
[7/150][55/105] Loss: 1.5687
[7/150][66/105] Loss: 1.5854
[7/150][77/105] Loss: 1.5494
[7/150][88/105] Loss: 1.5757
[7/150][99/105] Loss: 1.5494
Test set: Average loss: 0.0517, Accuracy: 1392/1670 (83%)
[8/150][0/105] Loss: 1.6025
[8/150][11/105] Loss: 1.5865
[8/150][22/105] Loss: 1.6244
[8/150][33/105] Loss: 1.5505
[8/150][44/105] Loss: 1.5696
[8/150][55/105] Loss: 1.5403
[8/150][66/105] Loss: 1.6716
[8/150][77/105] Loss: 1.5793
[8/150][88/105] Loss: 1.5098
[8/150][99/105] Loss: 1.5313
Test set: Average loss: 0.0511, Accuracy: 1426/1670 (85%)
[9/150][0/105] Loss: 1.6258
[9/150][11/105] Loss: 1.5070
[9/150][22/105] Loss: 1.4912
[9/150][33/105] Loss: 1.5586
[9/150][44/105] Loss: 1.5841
[9/150][55/105] Loss: 1.6069
[9/150][66/105] Loss: 1.5834
[9/150][77/105] Loss: 1.5203
[9/150][88/105] Loss: 1.5365
[9/150][99/105] Loss: 1.6176
Test set: Average loss: 0.0507, Accuracy: 1451/1670 (86%)
[10/150][0/105] Loss: 1.5868
[10/150][11/105] Loss: 1.5253
[10/150][22/105] Loss: 1.5305
[10/150][33/105] Loss: 1.5661
[10/150][44/105] Loss: 1.6075
[10/150][55/105] Loss: 1.5156
[10/150][66/105] Loss: 1.6020
[10/150][77/105] Loss: 1.4988
[10/150][88/105] Loss: 1.5060
[10/150][99/105] Loss: 1.5271
Test set: Average loss: 0.0511, Accuracy: 1426/1670 (85%)
[11/150][0/105] Loss: 1.6341
[11/150][11/105] Loss: 1.6094
[11/150][22/105] Loss: 1.5367
[11/150][33/105] Loss: 1.5009
[11/150][44/105] Loss: 1.4855
[11/150][55/105] Loss: 1.5590
[11/150][66/105] Loss: 1.5954
[11/150][77/105] Loss: 1.5502
[11/150][88/105] Loss: 1.5569
[11/150][99/105] Loss: 1.5189
Test set: Average loss: 0.0504, Accuracy: 1462/1670 (87%)
[12/150][0/105] Loss: 1.5027
[12/150][11/105] Loss: 1.5278
[12/150][22/105] Loss: 1.5124
[12/150][33/105] Loss: 1.4951
[12/150][44/105] Loss: 1.5881
[12/150][55/105] Loss: 1.5042
[12/150][66/105] Loss: 1.5554
[12/150][77/105] Loss: 1.5783
[12/150][88/105] Loss: 1.5716
[12/150][99/105] Loss: 1.5213
Test set: Average loss: 0.0502, Accuracy: 1469/1670 (87%)
[13/150][0/105] Loss: 1.4924
[13/150][11/105] Loss: 1.4905
[13/150][22/105] Loss: 1.5053
[13/150][33/105] Loss: 1.5026
[13/150][44/105] Loss: 1.5694
[13/150][55/105] Loss: 1.5678
[13/150][66/105] Loss: 1.5344
[13/150][77/105] Loss: 1.5582
[13/150][88/105] Loss: 1.5344
[13/150][99/105] Loss: 1.4987
Test set: Average loss: 0.0502, Accuracy: 1472/1670 (88%)
[14/150][0/105] Loss: 1.6408
[14/150][11/105] Loss: 1.5092
[14/150][22/105] Loss: 1.5520
[14/150][33/105] Loss: 1.5051
[14/150][44/105] Loss: 1.5429
[14/150][55/105] Loss: 1.5467
[14/150][66/105] Loss: 1.5056
[14/150][77/105] Loss: 1.5836
[14/150][88/105] Loss: 1.4841
[14/150][99/105] Loss: 1.5871
Test set: Average loss: 0.0503, Accuracy: 1474/1670 (88%)
[15/150][0/105] Loss: 1.4850
[15/150][11/105] Loss: 1.6241
[15/150][22/105] Loss: 1.4799
[15/150][33/105] Loss: 1.4993
[15/150][44/105] Loss: 1.5685
[15/150][55/105] Loss: 1.5126
[15/150][66/105] Loss: 1.5868
[15/150][77/105] Loss: 1.4674
[15/150][88/105] Loss: 1.5088
[15/150][99/105] Loss: 1.5880
Test set: Average loss: 0.0503, Accuracy: 1462/1670 (87%)
[16/150][0/105] Loss: 1.5006
[16/150][11/105] Loss: 1.5315
[16/150][22/105] Loss: 1.4757
[16/150][33/105] Loss: 1.4675
[16/150][44/105] Loss: 1.4677
[16/150][55/105] Loss: 1.5195
[16/150][66/105] Loss: 1.4953
[16/150][77/105] Loss: 1.5354
[16/150][88/105] Loss: 1.5415
[16/150][99/105] Loss: 1.5535
Test set: Average loss: 0.0504, Accuracy: 1475/1670 (88%)
[17/150][0/105] Loss: 1.5458
[17/150][11/105] Loss: 1.5189
[17/150][22/105] Loss: 1.5392
[17/150][33/105] Loss: 1.5091
[17/150][44/105] Loss: 1.5753
[17/150][55/105] Loss: 1.5380
[17/150][66/105] Loss: 1.5573
[17/150][77/105] Loss: 1.5400
[17/150][88/105] Loss: 1.4914
[17/150][99/105] Loss: 1.5376
Test set: Average loss: 0.0502, Accuracy: 1468/1670 (87%)
[18/150][0/105] Loss: 1.5339
[18/150][11/105] Loss: 1.5366
[18/150][22/105] Loss: 1.5877
[18/150][33/105] Loss: 1.4666
[18/150][44/105] Loss: 1.4690
[18/150][55/105] Loss: 1.4941
[18/150][66/105] Loss: 1.4874
[18/150][77/105] Loss: 1.5364
[18/150][88/105] Loss: 1.5220
[18/150][99/105] Loss: 1.4939
Test set: Average loss: 0.0500, Accuracy: 1480/1670 (88%)
[19/150][0/105] Loss: 1.4655
[19/150][11/105] Loss: 1.5578
[19/150][22/105] Loss: 1.4854
[19/150][33/105] Loss: 1.5046
[19/150][44/105] Loss: 1.5487
[19/150][55/105] Loss: 1.4642
[19/150][66/105] Loss: 1.5045
[19/150][77/105] Loss: 1.5548
[19/150][88/105] Loss: 1.5456
[19/150][99/105] Loss: 1.5155
Test set: Average loss: 0.0505, Accuracy: 1456/1670 (87%)
[20/150][0/105] Loss: 1.5058
[20/150][11/105] Loss: 1.5096
[20/150][22/105] Loss: 1.4939
[20/150][33/105] Loss: 1.4635
[20/150][44/105] Loss: 1.5657
[20/150][55/105] Loss: 1.5098
[20/150][66/105] Loss: 1.4652
[20/150][77/105] Loss: 1.5707
[20/150][88/105] Loss: 1.4966
[20/150][99/105] Loss: 1.4921
Test set: Average loss: 0.0498, Accuracy: 1500/1670 (89%)
[21/150][0/105] Loss: 1.5079
[21/150][11/105] Loss: 1.4854
[21/150][22/105] Loss: 1.4856
[21/150][33/105] Loss: 1.4700
[21/150][44/105] Loss: 1.4626
[21/150][55/105] Loss: 1.5666
[21/150][66/105] Loss: 1.4738
[21/150][77/105] Loss: 1.5353
[21/150][88/105] Loss: 1.4774
[21/150][99/105] Loss: 1.5185
Test set: Average loss: 0.0502, Accuracy: 1477/1670 (88%)
[22/150][0/105] Loss: 1.4964
[22/150][11/105] Loss: 1.4643
[22/150][22/105] Loss: 1.6315
[22/150][33/105] Loss: 1.4753
[22/150][44/105] Loss: 1.4617
[22/150][55/105] Loss: 1.5227
[22/150][66/105] Loss: 1.5567
[22/150][77/105] Loss: 1.4957
[22/150][88/105] Loss: 1.5561
[22/150][99/105] Loss: 1.4934
Test set: Average loss: 0.0500, Accuracy: 1482/1670 (88%)
[23/150][0/105] Loss: 1.4634
[23/150][11/105] Loss: 1.5263
[23/150][22/105] Loss: 1.5378
[23/150][33/105] Loss: 1.5256
[23/150][44/105] Loss: 1.5445
[23/150][55/105] Loss: 1.4618
[23/150][66/105] Loss: 1.5205
[23/150][77/105] Loss: 1.5008
[23/150][88/105] Loss: 1.5299
[23/150][99/105] Loss: 1.5290
Test set: Average loss: 0.0499, Accuracy: 1481/1670 (88%)
[24/150][0/105] Loss: 1.4751
[24/150][11/105] Loss: 1.4708
[24/150][22/105] Loss: 1.4647
[24/150][33/105] Loss: 1.4940
[24/150][44/105] Loss: 1.4801
[24/150][55/105] Loss: 1.5723
[24/150][66/105] Loss: 1.4718
[24/150][77/105] Loss: 1.5252
[24/150][88/105] Loss: 1.5050
[24/150][99/105] Loss: 1.5202
Test set: Average loss: 0.0496, Accuracy: 1503/1670 (90%)
[25/150][0/105] Loss: 1.4683
[25/150][11/105] Loss: 1.6706
[25/150][22/105] Loss: 1.5931
[25/150][33/105] Loss: 1.5982
[25/150][44/105] Loss: 1.5364
[25/150][55/105] Loss: 1.5831
[25/150][66/105] Loss: 1.4957
[25/150][77/105] Loss: 1.5039
[25/150][88/105] Loss: 1.5129
[25/150][99/105] Loss: 1.5302
Test set: Average loss: 0.0501, Accuracy: 1470/1670 (88%)
[26/150][0/105] Loss: 1.5085
[26/150][11/105] Loss: 1.5349
[26/150][22/105] Loss: 1.5177
[26/150][33/105] Loss: 1.4690
[26/150][44/105] Loss: 1.5256
[26/150][55/105] Loss: 1.5245
[26/150][66/105] Loss: 1.4662
[26/150][77/105] Loss: 1.4830
[26/150][88/105] Loss: 1.5043
[26/150][99/105] Loss: 1.4981
Test set: Average loss: 0.0503, Accuracy: 1463/1670 (87%)
[27/150][0/105] Loss: 1.5038
[27/150][11/105] Loss: 1.4804
[27/150][22/105] Loss: 1.5002
[27/150][33/105] Loss: 1.5256
[27/150][44/105] Loss: 1.4809
[27/150][55/105] Loss: 1.5344
[27/150][66/105] Loss: 1.5240
[27/150][77/105] Loss: 1.5315
[27/150][88/105] Loss: 1.5548
[27/150][99/105] Loss: 1.5058
Test set: Average loss: 0.0500, Accuracy: 1486/1670 (88%)
[28/150][0/105] Loss: 1.4679
[28/150][11/105] Loss: 1.5045
[28/150][22/105] Loss: 1.5001
[28/150][33/105] Loss: 1.4613
[28/150][44/105] Loss: 1.5025
[28/150][55/105] Loss: 1.6093
[28/150][66/105] Loss: 1.5013
[28/150][77/105] Loss: 1.4740
[28/150][88/105] Loss: 1.4672
[28/150][99/105] Loss: 1.4682
Test set: Average loss: 0.0499, Accuracy: 1478/1670 (88%)
[29/150][0/105] Loss: 1.5229
[29/150][11/105] Loss: 1.4802
[29/150][22/105] Loss: 1.5304
[29/150][33/105] Loss: 1.5314
[29/150][44/105] Loss: 1.4625
[29/150][55/105] Loss: 1.5224
[29/150][66/105] Loss: 1.5288
[29/150][77/105] Loss: 1.5499
[29/150][88/105] Loss: 1.5047
[29/150][99/105] Loss: 1.4648
Test set: Average loss: 0.0498, Accuracy: 1493/1670 (89%)
[30/150][0/105] Loss: 1.4718
[30/150][11/105] Loss: 1.4613
[30/150][22/105] Loss: 1.4662
[30/150][33/105] Loss: 1.4668
[30/150][44/105] Loss: 1.4802
[30/150][55/105] Loss: 1.5084
[30/150][66/105] Loss: 1.4989
[30/150][77/105] Loss: 1.4619
[30/150][88/105] Loss: 1.4764
[30/150][99/105] Loss: 1.5532
Test set: Average loss: 0.0500, Accuracy: 1484/1670 (88%)
[31/150][0/105] Loss: 1.4642
[31/150][11/105] Loss: 1.4829
[31/150][22/105] Loss: 1.4844
[31/150][33/105] Loss: 1.5681
[31/150][44/105] Loss: 1.4962
[31/150][55/105] Loss: 1.5006
[31/150][66/105] Loss: 1.5041
[31/150][77/105] Loss: 1.5011
[31/150][88/105] Loss: 1.5041
[31/150][99/105] Loss: 1.5305
Test set: Average loss: 0.0498, Accuracy: 1496/1670 (89%)
[32/150][0/105] Loss: 1.4969
[32/150][11/105] Loss: 1.5109
[32/150][22/105] Loss: 1.4934
[32/150][33/105] Loss: 1.5379
[32/150][44/105] Loss: 1.5343
[32/150][55/105] Loss: 1.4617
[32/150][66/105] Loss: 1.4924
[32/150][77/105] Loss: 1.4690
[32/150][88/105] Loss: 1.4671
[32/150][99/105] Loss: 1.5088
Test set: Average loss: 0.0497, Accuracy: 1497/1670 (89%)
[33/150][0/105] Loss: 1.4691
[33/150][11/105] Loss: 1.4752
[33/150][22/105] Loss: 1.5041
[33/150][33/105] Loss: 1.4668
[33/150][44/105] Loss: 1.4615
[33/150][55/105] Loss: 1.4959
[33/150][66/105] Loss: 1.4661
[33/150][77/105] Loss: 1.4886
[33/150][88/105] Loss: 1.4937
[33/150][99/105] Loss: 1.4837
Test set: Average loss: 0.0500, Accuracy: 1485/1670 (88%)
[34/150][0/105] Loss: 1.4654
[34/150][11/105] Loss: 1.5293
[34/150][22/105] Loss: 1.5921
[34/150][33/105] Loss: 1.5240
[34/150][44/105] Loss: 1.5588
[34/150][55/105] Loss: 1.5875
[34/150][66/105] Loss: 1.4954
[34/150][77/105] Loss: 1.5238
[34/150][88/105] Loss: 1.4978
[34/150][99/105] Loss: 1.4792
Test set: Average loss: 0.0506, Accuracy: 1441/1670 (86%)
[35/150][0/105] Loss: 1.5117
[35/150][11/105] Loss: 1.5240
[35/150][22/105] Loss: 1.5490
[35/150][33/105] Loss: 1.4647
[35/150][44/105] Loss: 1.5607
[35/150][55/105] Loss: 1.5088
[35/150][66/105] Loss: 1.4701
[35/150][77/105] Loss: 1.5767
[35/150][88/105] Loss: 1.4631
[35/150][99/105] Loss: 1.5163
^CProcess Process-144:
Traceback (most recent call last):
  File "main_ggnn.py", line 102, in <module>
    main(opt)
  File "main_ggnn.py", line 94, in main
    test(test_dataloader, net, criterion, optimizer, opt)
  File "/e/utils/test_ggnn.py", line 15, in test
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 57, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 57, in <listcomp>
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/e/utils/data/dataset.py", line 233, in __getitem__
    am = create_adjacency_matrix(self.data[index][0], self.n_node, self.n_edge_types)
  File "/e/utils/data/dataset.py", line 191, in create_adjacency_matrix
    a[tgt_idx-1][(e_type - 1) * n_nodes + src_idx - 1] =  1
KeyboardInterrupt
    adj_matrix = adj_matrix.cuda()
KeyboardInterrupt
(58%)
(74%)
(77%)
(79%)
(81%)
(81%)
(83%)
(83%)
(85%)
(85%)
(86%)
(86%)
(87%)
(87%)
(87%)
(87%)
(87%)
(87%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(88%)
(89%)
(89%)
(89%)
(89%)
(90%)
