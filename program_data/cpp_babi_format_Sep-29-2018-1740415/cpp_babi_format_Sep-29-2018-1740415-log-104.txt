Namespace(cuda=True, directory='program_data/cpp_babi_format_Sep-29-2018-1740415', is_training_ggnn=True, log_path='program_data/cpp_babi_format_Sep-29-2018-1740415/logs', lr=0.01, manualSeed=0, model_path='program_data/cpp_babi_format_Sep-29-2018-1740415/cpp_babi_format_Sep-29-2018-1740415-104.cpkl', n_classes=104, n_hidden=50, n_steps=5, niter=150, size_vocabulary=58, state_dim=5, test_batch_size=32, testing=False, train_batch_size=32, training=True, training_percentage=1.0, verbal=True, workers=0)
Random Seed:  0
  0% 0/104 [00:00<?, ?it/s]  1% 1/104 [00:00<00:24,  4.26it/s]  2% 2/104 [00:00<00:35,  2.85it/s]  3% 3/104 [00:01<00:32,  3.07it/s]  4% 4/104 [00:01<00:34,  2.92it/s]  5% 5/104 [00:01<00:36,  2.71it/s]  6% 6/104 [00:02<00:38,  2.56it/s]  7% 7/104 [00:02<00:39,  2.47it/s]  8% 8/104 [00:03<00:41,  2.30it/s]  9% 9/104 [00:03<00:46,  2.02it/s] 10% 10/104 [00:04<00:45,  2.06it/s] 11% 11/104 [00:04<00:43,  2.16it/s] 12% 12/104 [00:05<00:36,  2.49it/s] 12% 13/104 [00:05<00:37,  2.45it/s] 13% 14/104 [00:06<00:38,  2.31it/s] 14% 15/104 [00:06<00:35,  2.53it/s] 15% 16/104 [00:06<00:30,  2.89it/s] 16% 17/104 [00:07<00:37,  2.32it/s] 17% 18/104 [00:07<00:43,  1.97it/s] 18% 19/104 [00:08<00:39,  2.13it/s] 19% 20/104 [00:08<00:43,  1.95it/s] 20% 21/104 [00:09<00:40,  2.06it/s] 21% 22/104 [00:09<00:35,  2.30it/s] 22% 23/104 [00:09<00:30,  2.66it/s] 23% 24/104 [00:10<00:30,  2.62it/s] 24% 25/104 [00:10<00:37,  2.13it/s] 25% 26/104 [00:11<00:29,  2.62it/s] 26% 27/104 [00:11<00:31,  2.47it/s] 27% 28/104 [00:11<00:26,  2.84it/s] 28% 29/104 [00:12<00:24,  3.08it/s] 29% 30/104 [00:12<00:20,  3.57it/s] 30% 31/104 [00:12<00:22,  3.26it/s] 31% 32/104 [00:13<00:36,  1.98it/s] 32% 33/104 [00:13<00:30,  2.31it/s] 33% 34/104 [00:13<00:24,  2.83it/s] 34% 35/104 [00:14<00:25,  2.75it/s] 35% 36/104 [00:14<00:23,  2.86it/s] 36% 37/104 [00:14<00:22,  3.01it/s] 37% 38/104 [00:15<00:21,  3.11it/s] 38% 39/104 [00:15<00:25,  2.50it/s] 38% 40/104 [00:16<00:23,  2.68it/s] 39% 41/104 [00:17<00:39,  1.61it/s] 40% 42/104 [00:17<00:31,  1.95it/s] 41% 43/104 [00:17<00:25,  2.35it/s] 42% 44/104 [00:18<00:24,  2.48it/s] 43% 45/104 [00:18<00:20,  2.90it/s] 44% 46/104 [00:18<00:23,  2.51it/s] 45% 47/104 [00:19<00:19,  2.99it/s] 46% 48/104 [00:19<00:23,  2.39it/s] 47% 49/104 [00:20<00:20,  2.64it/s] 48% 50/104 [00:20<00:19,  2.80it/s] 49% 51/104 [00:21<00:32,  1.61it/s] 50% 52/104 [00:21<00:26,  1.96it/s] 51% 53/104 [00:22<00:22,  2.31it/s] 52% 54/104 [00:22<00:18,  2.77it/s] 53% 55/104 [00:22<00:19,  2.49it/s] 54% 56/104 [00:23<00:17,  2.74it/s] 55% 57/104 [00:23<00:16,  2.88it/s] 56% 58/104 [00:23<00:16,  2.81it/s] 57% 59/104 [00:24<00:19,  2.31it/s] 58% 60/104 [00:24<00:16,  2.66it/s] 59% 61/104 [00:24<00:13,  3.11it/s] 60% 62/104 [00:24<00:11,  3.54it/s] 61% 63/104 [00:25<00:13,  3.00it/s] 62% 64/104 [00:27<00:29,  1.35it/s] 62% 65/104 [00:27<00:23,  1.63it/s] 63% 66/104 [00:27<00:21,  1.73it/s] 64% 67/104 [00:28<00:17,  2.10it/s] 65% 68/104 [00:28<00:14,  2.42it/s] 66% 69/104 [00:28<00:16,  2.15it/s] 67% 70/104 [00:29<00:14,  2.34it/s] 68% 71/104 [00:29<00:14,  2.22it/s] 69% 72/104 [00:30<00:15,  2.01it/s] 70% 73/104 [00:30<00:14,  2.09it/s] 71% 74/104 [00:31<00:13,  2.16it/s] 72% 75/104 [00:31<00:12,  2.25it/s] 73% 76/104 [00:33<00:22,  1.27it/s] 74% 77/104 [00:33<00:17,  1.57it/s] 75% 78/104 [00:34<00:15,  1.72it/s] 76% 79/104 [00:34<00:12,  2.00it/s] 77% 80/104 [00:35<00:13,  1.82it/s] 78% 81/104 [00:35<00:11,  2.00it/s] 79% 82/104 [00:35<00:09,  2.30it/s] 80% 83/104 [00:36<00:09,  2.16it/s] 81% 84/104 [00:36<00:07,  2.56it/s] 82% 85/104 [00:36<00:07,  2.67it/s] 83% 86/104 [00:37<00:06,  2.83it/s] 84% 87/104 [00:37<00:05,  3.06it/s] 85% 88/104 [00:37<00:04,  3.44it/s] 86% 89/104 [00:37<00:04,  3.53it/s] 87% 90/104 [00:38<00:03,  3.72it/s] 88% 91/104 [00:38<00:03,  4.19it/s] 88% 92/104 [00:38<00:04,  2.94it/s] 89% 93/104 [00:39<00:03,  3.21it/s] 90% 94/104 [00:39<00:03,  3.21it/s] 91% 95/104 [00:39<00:02,  3.27it/s] 92% 96/104 [00:41<00:06,  1.31it/s] 93% 97/104 [00:41<00:04,  1.64it/s] 94% 98/104 [00:41<00:03,  1.97it/s] 95% 99/104 [00:42<00:02,  2.24it/s] 96% 100/104 [00:42<00:01,  2.29it/s] 97% 101/104 [00:43<00:01,  2.34it/s] 98% 102/104 [00:43<00:00,  2.24it/s] 99% 103/104 [00:43<00:00,  2.56it/s]100% 104/104 [00:44<00:00,  2.84it/s]
Number of all training data : 34627
Max node id : 54
  0% 0/104 [00:00<?, ?it/s]  1% 1/104 [00:00<00:14,  7.13it/s]  2% 2/104 [00:00<00:17,  5.72it/s]  3% 3/104 [00:00<00:15,  6.47it/s]  4% 4/104 [00:00<00:15,  6.52it/s]  5% 5/104 [00:00<00:15,  6.23it/s]  6% 6/104 [00:01<00:16,  6.01it/s]  7% 7/104 [00:01<00:16,  6.03it/s]  8% 8/104 [00:01<00:18,  5.30it/s]  9% 9/104 [00:01<00:19,  4.92it/s] 10% 10/104 [00:01<00:17,  5.28it/s] 11% 11/104 [00:02<00:18,  5.14it/s] 12% 12/104 [00:02<00:16,  5.71it/s] 12% 13/104 [00:02<00:14,  6.35it/s] 13% 14/104 [00:02<00:16,  5.53it/s] 14% 15/104 [00:02<00:15,  5.75it/s] 15% 16/104 [00:02<00:14,  6.13it/s] 16% 17/104 [00:02<00:14,  5.93it/s] 17% 18/104 [00:03<00:18,  4.58it/s] 18% 19/104 [00:03<00:17,  4.76it/s] 19% 20/104 [00:03<00:16,  5.15it/s] 20% 21/104 [00:03<00:16,  5.02it/s] 21% 22/104 [00:04<00:15,  5.31it/s] 22% 23/104 [00:04<00:13,  5.94it/s] 23% 24/104 [00:04<00:14,  5.66it/s] 24% 25/104 [00:04<00:13,  5.98it/s] 26% 27/104 [00:04<00:12,  6.10it/s] 27% 28/104 [00:04<00:11,  6.73it/s] 28% 29/104 [00:05<00:10,  7.04it/s] 30% 31/104 [00:05<00:10,  7.17it/s] 31% 32/104 [00:07<00:59,  1.20it/s] 32% 33/104 [00:07<00:44,  1.61it/s] 34% 35/104 [00:08<00:33,  2.09it/s] 35% 36/104 [00:08<00:25,  2.62it/s] 36% 37/104 [00:08<00:20,  3.23it/s] 37% 38/104 [00:08<00:17,  3.82it/s] 38% 39/104 [00:08<00:17,  3.67it/s] 38% 40/104 [00:09<00:15,  4.20it/s] 39% 41/104 [00:09<00:16,  3.89it/s] 40% 42/104 [00:09<00:13,  4.58it/s] 41% 43/104 [00:09<00:11,  5.34it/s] 42% 44/104 [00:09<00:11,  5.44it/s] 43% 45/104 [00:09<00:09,  6.27it/s] 44% 46/104 [00:10<00:11,  5.24it/s] 46% 48/104 [00:10<00:10,  5.18it/s] 47% 49/104 [00:10<00:09,  5.57it/s] 48% 50/104 [00:10<00:09,  5.84it/s] 49% 51/104 [00:11<00:10,  5.20it/s] 50% 52/104 [00:11<00:09,  5.75it/s] 51% 53/104 [00:11<00:08,  6.23it/s] 53% 55/104 [00:11<00:08,  6.10it/s] 54% 56/104 [00:11<00:07,  6.41it/s] 55% 57/104 [00:12<00:07,  6.50it/s] 56% 58/104 [00:12<00:07,  6.21it/s] 57% 59/104 [00:12<00:08,  5.03it/s] 58% 60/104 [00:12<00:07,  5.73it/s] 60% 62/104 [00:12<00:06,  6.67it/s] 61% 63/104 [00:12<00:06,  5.86it/s] 62% 64/104 [00:13<00:09,  4.20it/s] 62% 65/104 [00:13<00:08,  4.73it/s] 63% 66/104 [00:13<00:08,  4.53it/s] 64% 67/104 [00:13<00:07,  5.28it/s] 65% 68/104 [00:14<00:06,  5.77it/s] 66% 69/104 [00:14<00:07,  4.81it/s] 67% 70/104 [00:14<00:06,  5.19it/s] 68% 71/104 [00:14<00:06,  4.79it/s] 69% 72/104 [00:15<00:07,  4.19it/s] 70% 73/104 [00:15<00:07,  4.42it/s] 71% 74/104 [00:15<00:06,  4.56it/s] 72% 75/104 [00:15<00:06,  4.79it/s] 73% 76/104 [00:15<00:05,  4.83it/s] 74% 77/104 [00:15<00:05,  5.31it/s] 75% 78/104 [00:16<00:05,  4.93it/s] 76% 79/104 [00:16<00:04,  5.37it/s] 77% 80/104 [00:16<00:05,  4.45it/s] 78% 81/104 [00:16<00:04,  4.62it/s] 79% 82/104 [00:16<00:04,  5.21it/s] 80% 83/104 [00:17<00:04,  4.64it/s] 81% 84/104 [00:17<00:03,  5.43it/s] 82% 85/104 [00:17<00:03,  5.50it/s] 83% 86/104 [00:17<00:03,  5.72it/s] 84% 87/104 [00:17<00:02,  6.22it/s] 85% 88/104 [00:17<00:02,  6.93it/s] 86% 89/104 [00:20<00:12,  1.21it/s] 87% 90/104 [00:20<00:08,  1.61it/s] 88% 92/104 [00:20<00:05,  2.04it/s] 89% 93/104 [00:21<00:04,  2.61it/s] 90% 94/104 [00:21<00:03,  3.15it/s] 91% 95/104 [00:21<00:02,  3.75it/s] 92% 96/104 [00:21<00:01,  4.04it/s] 93% 97/104 [00:21<00:01,  4.72it/s] 94% 98/104 [00:21<00:01,  5.34it/s] 95% 99/104 [00:21<00:00,  5.63it/s] 96% 100/104 [00:22<00:00,  5.47it/s] 97% 101/104 [00:22<00:00,  5.29it/s] 98% 102/104 [00:22<00:00,  4.84it/s] 99% 103/104 [00:22<00:00,  5.42it/s]100% 104/104 [00:22<00:00,  5.93it/s]
Number of all testing data : 17367
Max node id : 57
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator stack because torch.onnx.symbolic.stack does not exist
  .format(op_name, op_name))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %98 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %161 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %224 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %287 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %350 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
[0/150][0/1083] Loss: 4.6418
[0/150][109/1083] Loss: 4.6415
[0/150][218/1083] Loss: 4.6491
[0/150][327/1083] Loss: 4.6183
[0/150][436/1083] Loss: 4.6049
[0/150][545/1083] Loss: 4.6349
[0/150][654/1083] Loss: 4.6495
[0/150][763/1083] Loss: 4.5944
[0/150][872/1083] Loss: 4.5958
[0/150][981/1083] Loss: 4.6282
Test set: Average loss: 0.1439, Accuracy: 1004/17367 (5%)
[1/150][0/1083] Loss: 4.5667
[1/150][109/1083] Loss: 4.5030
[1/150][218/1083] Loss: 4.5982
[1/150][327/1083] Loss: 4.6276
[1/150][436/1083] Loss: 4.6268
[1/150][545/1083] Loss: 4.6015
[1/150][654/1083] Loss: 4.6108
[1/150][763/1083] Loss: 4.6277
[1/150][872/1083] Loss: 4.5657
[1/150][981/1083] Loss: 4.6596
Test set: Average loss: 0.1439, Accuracy: 1008/17367 (5%)
[2/150][0/1083] Loss: 4.6292
[2/150][109/1083] Loss: 4.6584
[2/150][218/1083] Loss: 4.6591
[2/150][327/1083] Loss: 4.6084
[2/150][436/1083] Loss: 4.6019
[2/150][545/1083] Loss: 4.5708
[2/150][654/1083] Loss: 4.5974
[2/150][763/1083] Loss: 4.5666
[2/150][872/1083] Loss: 4.6188
[2/150][981/1083] Loss: 4.6290
Test set: Average loss: 0.1442, Accuracy: 859/17367 (4%)
[3/150][0/1083] Loss: 4.6317
[3/150][109/1083] Loss: 4.6286
[3/150][218/1083] Loss: 4.6540
[3/150][327/1083] Loss: 4.6599
[3/150][436/1083] Loss: 4.4773
[3/150][545/1083] Loss: 4.5995
[3/150][654/1083] Loss: 4.5643
[3/150][763/1083] Loss: 4.5769
[3/150][872/1083] Loss: 4.5996
[3/150][981/1083] Loss: 4.6285
Test set: Average loss: 0.1436, Accuracy: 1184/17367 (6%)
[4/150][0/1083] Loss: 4.5684
[4/150][109/1083] Loss: 4.5047
[4/150][218/1083] Loss: 4.6421
[4/150][327/1083] Loss: 4.5885
[4/150][436/1083] Loss: 4.5354
[4/150][545/1083] Loss: 4.6263
[4/150][654/1083] Loss: 4.5638
[4/150][763/1083] Loss: 4.6003
[4/150][872/1083] Loss: 4.6280
[4/150][981/1083] Loss: 4.6269
Test set: Average loss: 0.1435, Accuracy: 1231/17367 (7%)
[5/150][0/1083] Loss: 4.5971
[5/150][109/1083] Loss: 4.5954
[5/150][218/1083] Loss: 4.5670
[5/150][327/1083] Loss: 4.5679
[5/150][436/1083] Loss: 4.5663
[5/150][545/1083] Loss: 4.5348
[5/150][654/1083] Loss: 4.6445
[5/150][763/1083] Loss: 4.5664
[5/150][872/1083] Loss: 4.5970
[5/150][981/1083] Loss: 4.6012
Test set: Average loss: 0.1433, Accuracy: 1354/17367 (7%)
[6/150][0/1083] Loss: 4.6274
[6/150][109/1083] Loss: 4.6277
[6/150][218/1083] Loss: 4.5378
[6/150][327/1083] Loss: 4.5895
[6/150][436/1083] Loss: 4.6286
[6/150][545/1083] Loss: 4.5980
[6/150][654/1083] Loss: 4.5358
[6/150][763/1083] Loss: 4.5680
[6/150][872/1083] Loss: 4.5395
[6/150][981/1083] Loss: 4.5666
Test set: Average loss: 0.1434, Accuracy: 1315/17367 (7%)
[7/150][0/1083] Loss: 4.6320
[7/150][109/1083] Loss: 4.5349
[7/150][218/1083] Loss: 4.5977
[7/150][327/1083] Loss: 4.5678
[7/150][436/1083] Loss: 4.5968
[7/150][545/1083] Loss: 4.5898
[7/150][654/1083] Loss: 4.6292
[7/150][763/1083] Loss: 4.6288
[7/150][872/1083] Loss: 4.5973
[7/150][981/1083] Loss: 4.6597
Test set: Average loss: 0.1433, Accuracy: 1350/17367 (7%)
[8/150][0/1083] Loss: 4.6288
[8/150][109/1083] Loss: 4.5039
[8/150][218/1083] Loss: 4.6286
[8/150][327/1083] Loss: 4.5713
[8/150][436/1083] Loss: 4.6602
[8/150][545/1083] Loss: 4.5416
[8/150][654/1083] Loss: 4.6595
[8/150][763/1083] Loss: 4.5978
[8/150][872/1083] Loss: 4.5930
[8/150][981/1083] Loss: 4.5679
Test set: Average loss: 0.1432, Accuracy: 1401/17367 (8%)
[9/150][0/1083] Loss: 4.6288
[9/150][109/1083] Loss: 4.6057
[9/150][218/1083] Loss: 4.5670
[9/150][327/1083] Loss: 4.6136
[9/150][436/1083] Loss: 4.5666
[9/150][545/1083] Loss: 4.5352
[9/150][654/1083] Loss: 4.5668
[9/150][763/1083] Loss: 4.5959
[9/150][872/1083] Loss: 4.5664
[9/150][981/1083] Loss: 4.5789
Test set: Average loss: 0.1435, Accuracy: 1209/17367 (6%)
[10/150][0/1083] Loss: 4.6300
[10/150][109/1083] Loss: 4.5976
[10/150][218/1083] Loss: 4.6290
[10/150][327/1083] Loss: 4.5978
[10/150][436/1083] Loss: 4.5510
[10/150][545/1083] Loss: 4.5370
[10/150][654/1083] Loss: 4.6015
[10/150][763/1083] Loss: 4.5670
[10/150][872/1083] Loss: 4.5978
[10/150][981/1083] Loss: 4.5352
Test set: Average loss: 0.1431, Accuracy: 1473/17367 (8%)
[11/150][0/1083] Loss: 4.5978
[11/150][109/1083] Loss: 4.5977
[11/150][218/1083] Loss: 4.5350
[11/150][327/1083] Loss: 4.5665
[11/150][436/1083] Loss: 4.5331
[11/150][545/1083] Loss: 4.5561
[11/150][654/1083] Loss: 4.5975
[11/150][763/1083] Loss: 4.5976
[11/150][872/1083] Loss: 4.5975
[11/150][981/1083] Loss: 4.5695
Test set: Average loss: 0.1430, Accuracy: 1489/17367 (8%)
[12/150][0/1083] Loss: 4.5663
[12/150][109/1083] Loss: 4.6290
[12/150][218/1083] Loss: 4.5352
[12/150][327/1083] Loss: 4.6598
[12/150][436/1083] Loss: 4.6059
[12/150][545/1083] Loss: 4.5664
[12/150][654/1083] Loss: 4.5980
[12/150][763/1083] Loss: 4.5207
[12/150][872/1083] Loss: 4.5663
[12/150][981/1083] Loss: 4.5975
Test set: Average loss: 0.1431, Accuracy: 1449/17367 (8%)
[13/150][0/1083] Loss: 4.5659
[13/150][109/1083] Loss: 4.6603
[13/150][218/1083] Loss: 4.5951
[13/150][327/1083] Loss: 4.5099
[13/150][436/1083] Loss: 4.6290
[13/150][545/1083] Loss: 4.5975
[13/150][654/1083] Loss: 4.5350
[13/150][763/1083] Loss: 4.5482
[13/150][872/1083] Loss: 4.6597
[13/150][981/1083] Loss: 4.5665
Test set: Average loss: 0.1429, Accuracy: 1531/17367 (8%)
[14/150][0/1083] Loss: 4.5039
[14/150][109/1083] Loss: 4.5667
[14/150][218/1083] Loss: 4.5056
[14/150][327/1083] Loss: 4.5693
[14/150][436/1083] Loss: 4.4684
[14/150][545/1083] Loss: 4.5665
[14/150][654/1083] Loss: 4.5352
[14/150][763/1083] Loss: 4.5978
[14/150][872/1083] Loss: 4.6295
[14/150][981/1083] Loss: 4.5953
Test set: Average loss: 0.1430, Accuracy: 1501/17367 (8%)
[15/150][0/1083] Loss: 4.5661
[15/150][109/1083] Loss: 4.5041
[15/150][218/1083] Loss: 4.5669
[15/150][327/1083] Loss: 4.5345
[15/150][436/1083] Loss: 4.6290
[15/150][545/1083] Loss: 4.5978
[15/150][654/1083] Loss: 4.5972
[15/150][763/1083] Loss: 4.5353
[15/150][872/1083] Loss: 4.4734
[15/150][981/1083] Loss: 4.6163
Test set: Average loss: 0.1429, Accuracy: 1540/17367 (8%)
[16/150][0/1083] Loss: 4.5664
[16/150][109/1083] Loss: 4.5666
[16/150][218/1083] Loss: 4.5991
[16/150][327/1083] Loss: 4.4749
[16/150][436/1083] Loss: 4.5348
[16/150][545/1083] Loss: 4.5668
[16/150][654/1083] Loss: 4.5354
[16/150][763/1083] Loss: 4.5660
[16/150][872/1083] Loss: 4.5676
[16/150][981/1083] Loss: 4.5977
Test set: Average loss: 0.1430, Accuracy: 1534/17367 (8%)
[17/150][0/1083] Loss: 4.5662
[17/150][109/1083] Loss: 4.4844
[17/150][218/1083] Loss: 4.5367
[17/150][327/1083] Loss: 4.6480
[17/150][436/1083] Loss: 4.4932
[17/150][545/1083] Loss: 4.5667
[17/150][654/1083] Loss: 4.5676
[17/150][763/1083] Loss: 4.5287
[17/150][872/1083] Loss: 4.5566
[17/150][981/1083] Loss: 4.5369
Test set: Average loss: 0.1429, Accuracy: 1587/17367 (9%)
[18/150][0/1083] Loss: 4.5323
[18/150][109/1083] Loss: 4.5427
^CProcess Process-74:
Process Process-73:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 52, in _worker_loop
    r = index_queue.get()
  File "/opt/conda/lib/python3.6/multiprocessing/queues.py", line 335, in get
    res = self._reader.recv_bytes()
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/opt/conda/lib/python3.6/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt
Traceback (most recent call last):
  File "main_ggnn.py", line 102, in <module>
    main(opt)
  File "main_ggnn.py", line 93, in main
    train(epoch, train_dataloader, net, criterion, optimizer, opt, writer)
  File "/e/utils/train_ggnn.py", line 27, in train
    writer.add_graph(net, (init_input, adj_matrix), verbose=False)
  File "/opt/conda/lib/python3.6/site-packages/tensorboardX/writer.py", line 520, in add_graph
    self.file_writer.add_graph(graph(model, input_to_model, verbose))
  File "/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py", line 98, in graph
    torch.onnx._optimize_trace(trace, False)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py", line 30, in _optimize_trace
    trace.set_graph(utils._optimize_graph(trace.graph(), aten))
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 95, in _optimize_graph
    graph = torch._C._jit_pass_onnx(graph, aten)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py", line 40, in _run_symbolic_function
    return utils._run_symbolic_function(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 368, in _run_symbolic_function
    return fn(g, *inputs, **attrs)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic.py", line 150, in mul
    return g.op("Mul", self, _if_scalar_type_as(other, self), **_broadcast_if_scalar(other))
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 318, in _graph_op
    args = list(const_if_tensor(arg) for arg in raw_args)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 318, in <genexpr>
    args = list(const_if_tensor(arg) for arg in raw_args)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 313, in const_if_tensor
    elif isinstance(arg, torch._C.Value):
KeyboardInterrupt
(4%)
(5%)
(5%)
(6%)
(6%)
(7%)
(7%)
(7%)
(7%)
(8%)
(8%)
(8%)
(8%)
(8%)
(8%)
(8%)
(8%)
(9%)
