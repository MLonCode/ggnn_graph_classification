# $1, $2 - the dataset folder containing training/testing samples
# $3 - the propagation step size
lang=cpp
left=$1
right=$2
N_STEPS=${N_STEPS:-$3}
# derive the number of classes depending on the number of files in the training folder
nn=$(ls $left/train | wc -l | cut -f1 -d" ")
N_CLASSES=${N_CLASSES:-$nn}
MEMORY=${MEMORY:-128G}
BATCH_SIZE=${BATCH_SIZE:-128}
N_ITER=${N_ITER:-172}
N_HIDDEN=${N_HIDDEN:-50}
STATE_DIM=${STATE_DIM:-5}
PERCENTAGE=${PERCENTAGE:-1.0}

# the cached dataset
cache=$(dirname $left)/$(basename $left)-$N_CLASSES
# comment out if you want to resume computation
rm -f $cache-train.pkl
rm -f $cache-test.pkl

# the model file
model=$left/left-$(basename $left)-$N_CLASSES.cpkl
# comment out if you want to resume computation
#rm -f $model

# the log file
log=$left/left-$(basename $left)-log-$N_CLASSES.txt
# comment out if you want to reset the log
mkdir -p $left/logs
chmod o+w $left/logs

#chmod -R a+w $left
# use the docker to run training
proxy="--build-arg http_proxy=http://wwwcache.open.ac.uk:80 --build-arg https_proxy=http://wwwcache.open.ac.uk:80"
if [ "$http_proxy" == "" ]; then
 proxy=
fi
docker=nvidia-docker
cuda=--cuda
if [ "ip-172-31-34-92" == "$(hostname)" ]; then
 docker=docker
 cuda=
fi

docker build $proxy -t progress progress
/usr/bin/time -f %e \
  $docker run -v $(pwd):/e -w /e --shm-size $MEMORY --rm -it progress \
  python main_biggnn.py \
	$cuda \
	--training \
        --manualSeed 0 \
	--model_path $model \
        --left_directory $left \
        --right_directory $right \
	--n_classes $N_CLASSES \
	--n_steps $N_STEPS \
	--n_hidden $N_HIDDEN \
	--niter $N_ITER \
        --state_dim $STATE_DIM \
	--train_batch_size $BATCH_SIZE \
	--test_batch_size $BATCH_SIZE \
        --data_percentage $PERCENTAGE \
        --workers 2 \
	--log_path $left/logs \
  | tee -a $log
