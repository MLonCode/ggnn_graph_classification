Namespace(cuda=True, is_training_ggnn=False, left_directory='program_data/github_cpp_babi_format_Oct-15-2018-0000024', log_path='program_data/github_cpp_babi_format_Oct-15-2018-0000024/logs', loss=0, lr=0.01, manualSeed=0, model_path='program_data/github_cpp_babi_format_Oct-15-2018-0000024/github_cpp_babi_format_Oct-15-2018-0000024-50', n_classes=50, n_hidden=50, n_steps=8, niter=300, right_directory='program_data/cll_github_java_babi_format_Oct-15-2018-0000024', size_vocabulary=329, state_dim=5, test_batch_size=64, testing=False, train_batch_size=64, training=True, verbal=True, workers=2)
Random Seed:  0
Training Bi-GGNN with cross entropy loss................
Loading data...............
[0/300][0/98] Loss: 0.8883
[0/300][10/98] Loss: 0.7707
[0/300][20/98] Loss: 0.7976
[0/300][30/98] Loss: 0.8601
[0/300][40/98] Loss: 0.8289
[0/300][50/98] Loss: 0.8133
[0/300][60/98] Loss: 0.8133
[0/300][70/98] Loss: 0.8133
[0/300][80/98] Loss: 0.9070
[0/300][90/98] Loss: 0.8289
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[1/300][0/98] Loss: 0.8289
[1/300][10/98] Loss: 0.7351
[1/300][20/98] Loss: 0.8445
[1/300][30/98] Loss: 0.7664
[1/300][40/98] Loss: 0.8601
[1/300][50/98] Loss: 0.9070
[1/300][60/98] Loss: 0.7820
[1/300][70/98] Loss: 0.7820
[1/300][80/98] Loss: 0.7820
[1/300][90/98] Loss: 0.7976
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[2/300][0/98] Loss: 0.7820
n [2/300][10/98] Loss: 0.8445
[2/300][20/98] Loss: 0.7820
[2/300][30/98] Loss: 0.7976
[2/300][40/98] Loss: 0.7976
[2/300][50/98] Loss: 0.8133
[2/300][60/98] Loss: 0.7820
[2/300][70/98] Loss: 0.7664
[2/300][80/98] Loss: 0.7508
[2/300][90/98] Loss: 0.7195
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[3/300][0/98] Loss: 0.8914
[3/300][10/98] Loss: 0.8445
[3/300][20/98] Loss: 0.8133
[3/300][30/98] Loss: 0.8133
[3/300][40/98] Loss: 0.9383
[3/300][50/98] Loss: 0.8289
[3/300][60/98] Loss: 0.7820
[3/300][70/98] Loss: 0.7820
[3/300][80/98] Loss: 0.9383
[3/300][90/98] Loss: 0.8914
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[4/300][0/98] Loss: 0.8914
[4/300][10/98] Loss: 0.7664
[4/300][20/98] Loss: 0.8445
[4/300][30/98] Loss: 0.7195
[4/300][40/98] Loss: 0.8445
[4/300][50/98] Loss: 0.7508
[4/300][60/98] Loss: 0.8133
[4/300][70/98] Loss: 0.8289
[4/300][80/98] Loss: 0.8445
[4/300][90/98] Loss: 0.7351
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[5/300][0/98] Loss: 0.8289
[5/300][10/98] Loss: 0.7820
[5/300][20/98] Loss: 0.8133
[5/300][30/98] Loss: 0.9070
[5/300][40/98] Loss: 0.7820
[5/300][50/98] Loss: 0.7351
[5/300][60/98] Loss: 0.7664
[5/300][70/98] Loss: 0.8289
[5/300][80/98] Loss: 0.8133
[5/300][90/98] Loss: 0.8914
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[6/300][0/98] Loss: 0.8758
[6/300][10/98] Loss: 0.8601
[6/300][20/98] Loss: 0.7508
[6/300][30/98] Loss: 0.7976
[6/300][40/98] Loss: 0.6883
[6/300][50/98] Loss: 0.7820
[6/300][60/98] Loss: 0.7820
[6/300][70/98] Loss: 0.8133
[6/300][80/98] Loss: 0.7664
[6/300][90/98] Loss: 0.6570
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[7/300][0/98] Loss: 0.7976
[7/300][10/98] Loss: 0.8914
[7/300][20/98] Loss: 0.8289
[7/300][30/98] Loss: 0.7664
[7/300][40/98] Loss: 0.7195
[7/300][50/98] Loss: 0.8445
[7/300][60/98] Loss: 0.7195
[7/300][70/98] Loss: 0.7976
[7/300][80/98] Loss: 0.8758
[7/300][90/98] Loss: 0.7508
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[8/300][0/98] Loss: 0.7195
[8/300][10/98] Loss: 0.7351
[8/300][20/98] Loss: 0.7195
[8/300][30/98] Loss: 0.8914
[8/300][40/98] Loss: 0.8133
[8/300][50/98] Loss: 0.8445
[8/300][60/98] Loss: 0.7976
[8/300][70/98] Loss: 0.7976
[8/300][80/98] Loss: 0.8133
[8/300][90/98] Loss: 0.7195
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[9/300][0/98] Loss: 0.8601
[9/300][10/98] Loss: 0.7664
[9/300][20/98] Loss: 0.7351
[9/300][30/98] Loss: 0.7976
[9/300][40/98] Loss: 0.9226
[9/300][50/98] Loss: 0.6726
[9/300][60/98] Loss: 0.7039
[9/300][70/98] Loss: 0.7820
[9/300][80/98] Loss: 0.8758
[9/300][90/98] Loss: 0.8133
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[10/300][0/98] Loss: 0.8601
[10/300][10/98] Loss: 0.7195
[10/300][20/98] Loss: 0.7820
[10/300][30/98] Loss: 0.6883
[10/300][40/98] Loss: 0.8133
[10/300][50/98] Loss: 0.7664
[10/300][60/98] Loss: 0.7820
[10/300][70/98] Loss: 0.7976
[10/300][80/98] Loss: 0.8133
[10/300][90/98] Loss: 0.8289
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[11/300][0/98] Loss: 0.7820
[11/300][10/98] Loss: 0.8758
[11/300][20/98] Loss: 0.8289
[11/300][30/98] Loss: 0.8289
[11/300][40/98] Loss: 0.7508
[11/300][50/98] Loss: 0.7664
[11/300][60/98] Loss: 0.8601
[11/300][70/98] Loss: 0.7508
[11/300][80/98] Loss: 0.8133
[11/300][90/98] Loss: 0.7976
Saving model................
Test set: Average loss: 0.0128, Accuracy: 3115/6230 (50%)
[12/300][0/98] Loss: 0.8758
[12/300][10/98] Loss: 0.7664
[12/300][20/98] Loss: 0.7508
[12/300][30/98] Loss: 0.9070
[12/300][40/98] Loss: 0.8445
[12/300][50/98] Loss: 0.8133
[12/300][60/98] Loss: 0.7976
[12/300][70/98] Loss: 0.7508
[12/300][80/98] Loss: 0.8289
[12/300][90/98] Loss: 0.8914
Saving model................
^CProcess Process-52:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 61, in _worker_loop
    data_queue.put((idx, samples))
  File "/opt/conda/lib/python3.6/multiprocessing/queues.py", line 341, in put
    obj = _ForkingPickler.dumps(obj)
  File "/opt/conda/lib/python3.6/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 121, in reduce_storage
    fd, size = storage._share_fd_()
KeyboardInterrupt
Process Process-51:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 57, in _worker_loop
    samples = collate_fn([dataset[i] for i in batch_indices])
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 138, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 138, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 138, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 138, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 124, in default_collate
    return torch.stack([torch.from_numpy(b) for b in batch], 0)
KeyboardInterrupt
Traceback (most recent call last):
  File "main_biggnn.py", line 120, in <module>
    main(opt)
  File "main_biggnn.py", line 112, in main
    test(test_dataloader, net, criterion, optimizer, opt)
  File "/e/utils/test_biggnn.py", line 48, in test
    output = net(left_init_input, left_adj_matrix, right_init_input, right_adj_matrix)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/e/utils/model.py", line 232, in forward
    right_output = self.ggnn(right_prop_state, right_A)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/e/utils/model.py", line 170, in forward
    out_states.append(self.out_fcs[i](prop_state))
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 491, in __call__
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 55, in forward
    return F.linear(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py", line 994, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
(50%)
