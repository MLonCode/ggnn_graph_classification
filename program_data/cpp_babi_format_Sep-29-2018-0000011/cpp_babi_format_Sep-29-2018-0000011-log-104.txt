Namespace(cuda=True, directory='program_data/cpp_babi_format_Sep-29-2018-0000011', is_training_ggnn=True, log_path='program_data/cpp_babi_format_Sep-29-2018-0000011/logs', lr=0.01, manualSeed=0, model_path='program_data/cpp_babi_format_Sep-29-2018-0000011/cpp_babi_format_Sep-29-2018-0000011-104.cpkl', n_classes=104, n_hidden=50, n_steps=5, niter=150, size_vocabulary=354, state_dim=5, test_batch_size=32, testing=False, train_batch_size=128, training=True, training_percentage=1.0, verbal=True, workers=2)
Random Seed:  0
Number of all training data : 34627
Max node id : 348
Number of all testing data : 17367
Max node id : 353
Using the saved model....
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator stack because torch.onnx.symbolic.stack does not exist
  .format(op_name, op_name))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %68 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %113 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %158 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %203 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %248 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
[0/150][0/271] Loss: 4.2023
[0/150][28/271] Loss: 4.1265
[0/150][56/271] Loss: 4.1946
[0/150][84/271] Loss: 4.1899
[0/150][112/271] Loss: 4.1785
[0/150][140/271] Loss: 4.1297
[0/150][168/271] Loss: 4.1530
[0/150][196/271] Loss: 4.1174
[0/150][224/271] Loss: 4.2086
[0/150][252/271] Loss: 4.2188
Test set: Average loss: 0.1321, Accuracy: 7599/17367 (43%)
[1/150][0/271] Loss: 4.1557
[1/150][28/271] Loss: 4.1537
[1/150][56/271] Loss: 4.1848
[1/150][84/271] Loss: 4.1668
[1/150][112/271] Loss: 4.1578
[1/150][140/271] Loss: 4.1549
[1/150][168/271] Loss: 4.1614
[1/150][196/271] Loss: 4.1491
[1/150][224/271] Loss: 4.0660
[1/150][252/271] Loss: 4.1442
Test set: Average loss: 0.1318, Accuracy: 7790/17367 (44%)
[2/150][0/271] Loss: 4.1795
[2/150][28/271] Loss: 4.1389
[2/150][56/271] Loss: 4.1307
[2/150][84/271] Loss: 4.1858
[2/150][112/271] Loss: 4.1990
[2/150][140/271] Loss: 4.1870
[2/150][168/271] Loss: 4.1614
[2/150][196/271] Loss: 4.1557
[2/150][224/271] Loss: 4.1776
[2/150][252/271] Loss: 4.0815
Test set: Average loss: 0.1317, Accuracy: 7813/17367 (44%)
[3/150][0/271] Loss: 4.1816
[3/150][28/271] Loss: 4.1743
[3/150][56/271] Loss: 4.1063
[3/150][84/271] Loss: 4.1542
[3/150][112/271] Loss: 4.1058
[3/150][140/271] Loss: 4.1039
[3/150][168/271] Loss: 4.1377
[3/150][196/271] Loss: 4.2283
[3/150][224/271] Loss: 4.1944
[3/150][252/271] Loss: 4.1294
Test set: Average loss: 0.1318, Accuracy: 7754/17367 (44%)
[4/150][0/271] Loss: 4.1146
[4/150][28/271] Loss: 4.1720
[4/150][56/271] Loss: 4.1158
[4/150][84/271] Loss: 4.0925
[4/150][112/271] Loss: 4.1335
[4/150][140/271] Loss: 4.2017
[4/150][168/271] Loss: 4.1527
[4/150][196/271] Loss: 4.1694
[4/150][224/271] Loss: 4.1262
[4/150][252/271] Loss: 4.0993
Test set: Average loss: 0.1318, Accuracy: 7793/17367 (44%)
[5/150][0/271] Loss: 4.1119
[5/150][28/271] Loss: 4.1394
[5/150][56/271] Loss: 4.2411
[5/150][84/271] Loss: 4.1614
[5/150][112/271] Loss: 4.1365
[5/150][140/271] Loss: 4.1971
[5/150][168/271] Loss: 4.1885
[5/150][196/271] Loss: 4.2050
[5/150][224/271] Loss: 4.1615
[5/150][252/271] Loss: 4.2006
Test set: Average loss: 0.1318, Accuracy: 7767/17367 (44%)
[6/150][0/271] Loss: 4.1872
[6/150][28/271] Loss: 4.1875
[6/150][56/271] Loss: 4.1631
[6/150][84/271] Loss: 4.1507
[6/150][112/271] Loss: 4.2469
[6/150][140/271] Loss: 4.1493
[6/150][168/271] Loss: 4.2508
[6/150][196/271] Loss: 4.1700
[6/150][224/271] Loss: 4.1253
[6/150][252/271] Loss: 4.1150
Test set: Average loss: 0.1318, Accuracy: 7750/17367 (44%)
[7/150][0/271] Loss: 4.1316
[7/150][28/271] Loss: 4.1455
[7/150][56/271] Loss: 4.1912
[7/150][84/271] Loss: 4.1512
[7/150][112/271] Loss: 4.1683
[7/150][140/271] Loss: 4.1612
[7/150][168/271] Loss: 4.1433
[7/150][196/271] Loss: 4.1324
[7/150][224/271] Loss: 4.0993
[7/150][252/271] Loss: 4.1615
Test set: Average loss: 0.1317, Accuracy: 7819/17367 (45%)
[8/150][0/271] Loss: 4.1644
[8/150][28/271] Loss: 4.1354
[8/150][56/271] Loss: 4.2149
[8/150][84/271] Loss: 4.1578
[8/150][112/271] Loss: 4.1563
[8/150][140/271] Loss: 4.2204
[8/150][168/271] Loss: 4.1470
[8/150][196/271] Loss: 4.1852
[8/150][224/271] Loss: 4.1561
[8/150][252/271] Loss: 4.1123
Test set: Average loss: 0.1317, Accuracy: 7807/17367 (44%)
[9/150][0/271] Loss: 4.0737
[9/150][28/271] Loss: 4.1751
[9/150][56/271] Loss: 4.0857
[9/150][84/271] Loss: 4.1689
[9/150][112/271] Loss: 4.1315
[9/150][140/271] Loss: 4.1664
[9/150][168/271] Loss: 4.1302
[9/150][196/271] Loss: 4.1950
[9/150][224/271] Loss: 4.1939
[9/150][252/271] Loss: 4.1566
Test set: Average loss: 0.1318, Accuracy: 7799/17367 (44%)
[10/150][0/271] Loss: 4.0783
[10/150][28/271] Loss: 4.2174
[10/150][56/271] Loss: 4.0832
[10/150][84/271] Loss: 4.1852
[10/150][112/271] Loss: 4.1604
[10/150][140/271] Loss: 4.2427
[10/150][168/271] Loss: 4.1383
[10/150][196/271] Loss: 4.1547
[10/150][224/271] Loss: 4.1065
[10/150][252/271] Loss: 4.1482
Test set: Average loss: 0.1317, Accuracy: 7800/17367 (44%)
[11/150][0/271] Loss: 4.1882
[11/150][28/271] Loss: 4.1293
[11/150][56/271] Loss: 4.2063
[11/150][84/271] Loss: 4.2010
[11/150][112/271] Loss: 4.1396
[11/150][140/271] Loss: 4.1231
[11/150][168/271] Loss: 4.1394
[11/150][196/271] Loss: 4.2089
[11/150][224/271] Loss: 4.2214
[11/150][252/271] Loss: 4.1157
Test set: Average loss: 0.1316, Accuracy: 7854/17367 (45%)
[12/150][0/271] Loss: 4.1495
[12/150][28/271] Loss: 4.1788
[12/150][56/271] Loss: 4.1141
[12/150][84/271] Loss: 4.1635
[12/150][112/271] Loss: 4.1961
[12/150][140/271] Loss: 4.1586
[12/150][168/271] Loss: 4.1919
[12/150][196/271] Loss: 4.1538
[12/150][224/271] Loss: 4.1360
[12/150][252/271] Loss: 4.1790
Test set: Average loss: 0.1318, Accuracy: 7769/17367 (44%)
[13/150][0/271] Loss: 4.2513
[13/150][28/271] Loss: 4.1754
[13/150][56/271] Loss: 4.1815
[13/150][84/271] Loss: 4.1987
[13/150][112/271] Loss: 4.0911
[13/150][140/271] Loss: 4.1112
[13/150][168/271] Loss: 4.1258
[13/150][196/271] Loss: 4.2208
[13/150][224/271] Loss: 4.2443
[13/150][252/271] Loss: 4.2263
Test set: Average loss: 0.1317, Accuracy: 7802/17367 (44%)
[14/150][0/271] Loss: 4.1693
[14/150][28/271] Loss: 4.1636
[14/150][56/271] Loss: 4.1778
[14/150][84/271] Loss: 4.1229
[14/150][112/271] Loss: 4.2216
[14/150][140/271] Loss: 4.1081
[14/150][168/271] Loss: 4.1716
[14/150][196/271] Loss: 4.2335
[14/150][224/271] Loss: 4.1837
[14/150][252/271] Loss: 4.1857
Test set: Average loss: 0.1317, Accuracy: 7803/17367 (44%)
[15/150][0/271] Loss: 4.0522
[15/150][28/271] Loss: 4.1656
[15/150][56/271] Loss: 4.1905
[15/150][84/271] Loss: 4.0987
[15/150][112/271] Loss: 4.1681
[15/150][140/271] Loss: 4.2081
[15/150][168/271] Loss: 4.0189
[15/150][196/271] Loss: 4.0845
[15/150][224/271] Loss: 4.2187
[15/150][252/271] Loss: 4.1827
Test set: Average loss: 0.1317, Accuracy: 7839/17367 (45%)
[16/150][0/271] Loss: 4.1359
[16/150][28/271] Loss: 4.1300
[16/150][56/271] Loss: 4.1707
[16/150][84/271] Loss: 4.1026
[16/150][112/271] Loss: 4.1230
[16/150][140/271] Loss: 4.2076
[16/150][168/271] Loss: 4.1486
[16/150][196/271] Loss: 4.1542
[16/150][224/271] Loss: 4.1568
[16/150][252/271] Loss: 4.1763
Test set: Average loss: 0.1317, Accuracy: 7839/17367 (45%)
[17/150][0/271] Loss: 4.2255
[17/150][28/271] Loss: 4.1843
[17/150][56/271] Loss: 4.1749
[17/150][84/271] Loss: 4.2304
[17/150][112/271] Loss: 4.2414
[17/150][140/271] Loss: 4.1684
[17/150][168/271] Loss: 4.1147
[17/150][196/271] Loss: 4.0860
[17/150][224/271] Loss: 4.0925
[17/150][252/271] Loss: 4.1174
Test set: Average loss: 0.1316, Accuracy: 7882/17367 (45%)
[18/150][0/271] Loss: 4.2328
[18/150][28/271] Loss: 4.1613
[18/150][56/271] Loss: 4.1306
[18/150][84/271] Loss: 4.1216
[18/150][112/271] Loss: 4.1776
[18/150][140/271] Loss: 4.1349
[18/150][168/271] Loss: 4.1623
[18/150][196/271] Loss: 4.1544
[18/150][224/271] Loss: 4.1026
[18/150][252/271] Loss: 4.1492
Test set: Average loss: 0.1318, Accuracy: 7754/17367 (44%)
[19/150][0/271] Loss: 4.2005
[19/150][28/271] Loss: 4.1526
[19/150][56/271] Loss: 4.1829
[19/150][84/271] Loss: 4.1454
[19/150][112/271] Loss: 4.1468
[19/150][140/271] Loss: 4.1558
[19/150][168/271] Loss: 4.1286
[19/150][196/271] Loss: 4.2161
[19/150][224/271] Loss: 4.2092
[19/150][252/271] Loss: 4.1657
Test set: Average loss: 0.1317, Accuracy: 7824/17367 (45%)
[20/150][0/271] Loss: 4.1544
[20/150][28/271] Loss: 4.0891
[20/150][56/271] Loss: 4.1247
[20/150][84/271] Loss: 4.1683
[20/150][112/271] Loss: 4.1290
[20/150][140/271] Loss: 4.0861
[20/150][168/271] Loss: 4.2062
[20/150][196/271] Loss: 4.2245
[20/150][224/271] Loss: 4.1896
[20/150][252/271] Loss: 4.0811
Test set: Average loss: 0.1318, Accuracy: 7784/17367 (44%)
[21/150][0/271] Loss: 4.1070
[21/150][28/271] Loss: 4.1787
[21/150][56/271] Loss: 4.1436
[21/150][84/271] Loss: 4.1884
[21/150][112/271] Loss: 4.1983
[21/150][140/271] Loss: 4.2073
[21/150][168/271] Loss: 4.1522
[21/150][196/271] Loss: 4.1787
[21/150][224/271] Loss: 4.2143
[21/150][252/271] Loss: 4.2010
Test set: Average loss: 0.1316, Accuracy: 7863/17367 (45%)
[22/150][0/271] Loss: 4.1339
[22/150][28/271] Loss: 4.1917
[22/150][56/271] Loss: 4.1354
[22/150][84/271] Loss: 4.1808
[22/150][112/271] Loss: 4.1521
[22/150][140/271] Loss: 4.1551
[22/150][168/271] Loss: 4.0909
[22/150][196/271] Loss: 4.1230
[22/150][224/271] Loss: 4.1706
[22/150][252/271] Loss: 4.0550
Test set: Average loss: 0.1317, Accuracy: 7814/17367 (44%)
[23/150][0/271] Loss: 4.0918
