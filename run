# $1 - the dataset folder containing training/testing samples
# $2 - the size of vocabulary, start with a small number and increase it if you get errors
lang=cpp
directory=$1
MEMORY=${MEMORY:-128G}
TRAIN_SIZE=${TRAIN_SIZE:-128}

# derive the number of classes depending on the number of files in the training folder
nn=$(ls $directory/train | wc -l | cut -f1 -d" ")
n=${2:-$nn}
m=${3:-2}
s=${4:-10}

# the cached dataset
cache=$(dirname $directory)/$(basename $directory)-$n
# comment out if you want to resume computation
rm -f $cache-train.pkl
rm -f $cache-test.pkl

# the model file
model=$directory/$n.cpkl
# comment out if you want to resume computation

# the log file
log=$directory/log-$n.txt
mkdir -p $directory/logs

chmod -R o+w $directory

# use the docker to run training
proxy="--build-arg http_proxy=http://wwwcache.open.ac.uk:80 --build-arg https_proxy=http://wwwcache.open.ac.uk:80"
if [ "$http_proxy" == "" ]; then
 proxy=
fi
docker=nvidia-docker
cuda=--cuda
if [ "ip-172-31-34-92" == "$(hostname)" ]; then
 docker=docker
 cuda=
fi

docker build $proxy -t progress progress
  NV_GPU=1 /usr/bin/time -f %e \
  $docker run -v $(pwd):/e -w /e --shm-size $MEMORY --rm -it progress \
  python main_ggnn.py \
	$cuda \
	--training \
        --manualSeed 0 \
	--model_path $model --directory $directory \
	--n_classes $n \
	--n_steps $5 \
	--n_hidden 50 \
	--niter $4 \
	--train_batch_size ${TRAIN_SIZE} \
	--test_batch_size ${TRAIN_SIZE} \
        --workers 2 \
	--log_path $directory/logs \
  | tee -a $log
