Namespace(cuda=True, directory='program_data/cpp_babi_format_Sep-29-2018-0000005', is_training_ggnn=True, log_path='program_data/cpp_babi_format_Sep-29-2018-0000005/logs', lr=0.01, manualSeed=0, model_path='program_data/cpp_babi_format_Sep-29-2018-0000005/cpp_babi_format_Sep-29-2018-0000005-104.cpkl', n_classes=104, n_hidden=50, n_steps=5, niter=150, size_vocabulary=56, state_dim=5, test_batch_size=32, testing=False, train_batch_size=32, training=True, training_percentage=1.0, verbal=True, workers=0)
Random Seed:  0
  0% 0/104 [00:00<?, ?it/s]  4% 4/104 [00:00<00:03, 28.32it/s]  8% 8/104 [00:00<00:03, 27.80it/s] 12% 12/104 [00:00<00:03, 26.78it/s] 15% 16/104 [00:00<00:03, 28.73it/s] 18% 19/104 [00:00<00:03, 25.45it/s] 21% 22/104 [00:00<00:03, 23.13it/s] 25% 26/104 [00:00<00:03, 25.50it/s] 28% 29/104 [00:01<00:03, 22.62it/s] 32% 33/104 [00:01<00:02, 24.84it/s] 35% 36/104 [00:01<00:03, 21.33it/s] 38% 40/104 [00:01<00:02, 24.02it/s] 42% 44/104 [00:01<00:02, 26.54it/s] 45% 47/104 [00:01<00:02, 20.61it/s] 49% 51/104 [00:02<00:02, 23.01it/s] 53% 55/104 [00:02<00:01, 25.43it/s] 56% 58/104 [00:02<00:02, 19.30it/s] 60% 62/104 [00:02<00:01, 22.22it/s] 63% 66/104 [00:02<00:01, 24.60it/s] 67% 70/104 [00:02<00:01, 26.49it/s] 70% 73/104 [00:03<00:01, 18.00it/s] 74% 77/104 [00:03<00:01, 20.58it/s] 78% 81/104 [00:03<00:00, 23.13it/s] 82% 85/104 [00:03<00:00, 25.46it/s] 86% 89/104 [00:03<00:00, 18.31it/s] 89% 93/104 [00:03<00:00, 21.22it/s] 93% 97/104 [00:04<00:00, 24.13it/s] 97% 101/104 [00:04<00:00, 26.44it/s]100% 104/104 [00:04<00:00, 24.51it/s]
Number of all training data : 34627
Max node id : 52
  0% 0/104 [00:00<?, ?it/s]  6% 6/104 [00:00<00:01, 54.33it/s] 11% 11/104 [00:00<00:03, 28.60it/s] 17% 18/104 [00:00<00:02, 34.50it/s] 24% 25/104 [00:00<00:01, 40.14it/s] 31% 32/104 [00:00<00:01, 45.38it/s] 38% 39/104 [00:00<00:01, 49.15it/s] 44% 46/104 [00:01<00:01, 52.81it/s] 51% 53/104 [00:01<00:00, 56.63it/s] 58% 60/104 [00:01<00:00, 58.81it/s] 64% 67/104 [00:01<00:01, 34.27it/s] 71% 74/104 [00:01<00:00, 39.84it/s] 78% 81/104 [00:01<00:00, 45.02it/s] 85% 88/104 [00:01<00:00, 49.80it/s] 91% 95/104 [00:02<00:00, 53.88it/s] 98% 102/104 [00:02<00:00, 57.63it/s]100% 104/104 [00:02<00:00, 47.33it/s]
Number of all testing data : 17367
Max node id : 55
/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py:365: UserWarning: ONNX export failed on ATen operator stack because torch.onnx.symbolic.stack does not exist
  .format(op_name, op_name))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %58 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %97 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %136 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %175 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py:43: UserWarning: Error getting attributes of node %214 : Dynamic = onnx::Constant[value={1}](), scope: GGNN/Propogator[propogator], error is VariableType::ID() not implemented
  warnings.warn("Error getting attributes of node {}, error is {}".format(attrs, e))
[0/150][0/1083] Loss: 4.6478
[0/150][109/1083] Loss: 4.6518
[0/150][218/1083] Loss: 4.4848
[0/150][327/1083] Loss: 4.5887
[0/150][436/1083] Loss: 4.6143
[0/150][545/1083] Loss: 4.5080
[0/150][654/1083] Loss: 4.6078
[0/150][763/1083] Loss: 4.5769
[0/150][872/1083] Loss: 4.5710
[0/150][981/1083] Loss: 4.5363
Test set: Average loss: 0.1424, Accuracy: 1862/17367 (10%)
[1/150][0/1083] Loss: 4.4596
[1/150][109/1083] Loss: 4.6282
[1/150][218/1083] Loss: 4.4985
[1/150][327/1083] Loss: 4.5695
[1/150][436/1083] Loss: 4.4799
[1/150][545/1083] Loss: 4.4514
[1/150][654/1083] Loss: 4.5387
[1/150][763/1083] Loss: 4.4438
[1/150][872/1083] Loss: 4.5032
[1/150][981/1083] Loss: 4.4221
Test set: Average loss: 0.1422, Accuracy: 2029/17367 (11%)
[2/150][0/1083] Loss: 4.6026
[2/150][109/1083] Loss: 4.6029
[2/150][218/1083] Loss: 4.5109
[2/150][327/1083] Loss: 4.5357
[2/150][436/1083] Loss: 4.5742
[2/150][545/1083] Loss: 4.5555
[2/150][654/1083] Loss: 4.5304
[2/150][763/1083] Loss: 4.5984
[2/150][872/1083] Loss: 4.5349
[2/150][981/1083] Loss: 4.5087
Test set: Average loss: 0.1419, Accuracy: 2131/17367 (12%)
[3/150][0/1083] Loss: 4.5014
[3/150][109/1083] Loss: 4.6191
[3/150][218/1083] Loss: 4.4823
[3/150][327/1083] Loss: 4.5676
[3/150][436/1083] Loss: 4.5659
[3/150][545/1083] Loss: 4.5676
[3/150][654/1083] Loss: 4.5465
[3/150][763/1083] Loss: 4.5383
[3/150][872/1083] Loss: 4.4973
[3/150][981/1083] Loss: 4.5675
Test set: Average loss: 0.1420, Accuracy: 2084/17367 (11%)
[4/150][0/1083] Loss: 4.5305
[4/150][109/1083] Loss: 4.5708
[4/150][218/1083] Loss: 4.5068
[4/150][327/1083] Loss: 4.5080
[4/150][436/1083] Loss: 4.6248
[4/150][545/1083] Loss: 4.5353
[4/150][654/1083] Loss: 4.4288
[4/150][763/1083] Loss: 4.5657
[4/150][872/1083] Loss: 4.5351
[4/150][981/1083] Loss: 4.5971
Test set: Average loss: 0.1420, Accuracy: 2067/17367 (11%)
[5/150][0/1083] Loss: 4.5827
[5/150][109/1083] Loss: 4.5979
[5/150][218/1083] Loss: 4.5407
[5/150][327/1083] Loss: 4.4715
[5/150][436/1083] Loss: 4.4416
[5/150][545/1083] Loss: 4.6015
[5/150][654/1083] Loss: 4.4441
[5/150][763/1083] Loss: 4.5982
[5/150][872/1083] Loss: 4.6601
[5/150][981/1083] Loss: 4.5336
Test set: Average loss: 0.1421, Accuracy: 2039/17367 (11%)
[6/150][0/1083] Loss: 4.5031
[6/150][109/1083] Loss: 4.6289
[6/150][218/1083] Loss: 4.5425
[6/150][327/1083] Loss: 4.4977
[6/150][436/1083] Loss: 4.5658
[6/150][545/1083] Loss: 4.5884
[6/150][654/1083] Loss: 4.5448
[6/150][763/1083] Loss: 4.5148
[6/150][872/1083] Loss: 4.5356
[6/150][981/1083] Loss: 4.5042
Test set: Average loss: 0.1418, Accuracy: 2189/17367 (12%)
[7/150][0/1083] Loss: 4.5049
[7/150][109/1083] Loss: 4.5965
[7/150][218/1083] Loss: 4.5044
[7/150][327/1083] Loss: 4.5663
[7/150][436/1083] Loss: 4.4141
[7/150][545/1083] Loss: 4.6127
[7/150][654/1083] Loss: 4.5356
[7/150][763/1083] Loss: 4.4082
[7/150][872/1083] Loss: 4.4426
[7/150][981/1083] Loss: 4.5356
Test set: Average loss: 0.1419, Accuracy: 2107/17367 (12%)
[8/150][0/1083] Loss: 4.5035
[8/150][109/1083] Loss: 4.6156
[8/150][218/1083] Loss: 4.4581
[8/150][327/1083] Loss: 4.5574
[8/150][436/1083] Loss: 4.5011
[8/150][545/1083] Loss: 4.5009
[8/150][654/1083] Loss: 4.5667
[8/150][763/1083] Loss: 4.5307
[8/150][872/1083] Loss: 4.5710
[8/150][981/1083] Loss: 4.5979
Test set: Average loss: 0.1417, Accuracy: 2218/17367 (12%)
[9/150][0/1083] Loss: 4.4728
[9/150][109/1083] Loss: 4.4924
[9/150][218/1083] Loss: 4.5090
[9/150][327/1083] Loss: 4.6590
[9/150][436/1083] Loss: 4.6600
[9/150][545/1083] Loss: 4.5975
[9/150][654/1083] Loss: 4.5960
[9/150][763/1083] Loss: 4.5182
[9/150][872/1083] Loss: 4.5660
[9/150][981/1083] Loss: 4.5046
Test set: Average loss: 0.1419, Accuracy: 2112/17367 (12%)
[10/150][0/1083] Loss: 4.6006
[10/150][109/1083] Loss: 4.5541
[10/150][218/1083] Loss: 4.5710
[10/150][327/1083] Loss: 4.5043
[10/150][436/1083] Loss: 4.5253
[10/150][545/1083] Loss: 4.5430
[10/150][654/1083] Loss: 4.5100
[10/150][763/1083] Loss: 4.5669
[10/150][872/1083] Loss: 4.5518
[10/150][981/1083] Loss: 4.4720
Test set: Average loss: 0.1421, Accuracy: 2030/17367 (11%)
[11/150][0/1083] Loss: 4.5341
[11/150][109/1083] Loss: 4.4812
[11/150][218/1083] Loss: 4.4786
[11/150][327/1083] Loss: 4.5967
[11/150][436/1083] Loss: 4.6250
[11/150][545/1083] Loss: 4.3795
[11/150][654/1083] Loss: 4.5430
[11/150][763/1083] Loss: 4.4721
[11/150][872/1083] Loss: 4.5869
[11/150][981/1083] Loss: 4.5340
Test set: Average loss: 0.1420, Accuracy: 2085/17367 (12%)
[12/150][0/1083] Loss: 4.4682
[12/150][109/1083] Loss: 4.5622
[12/150][218/1083] Loss: 4.5037
[12/150][327/1083] Loss: 4.5055
[12/150][436/1083] Loss: 4.5271
[12/150][545/1083] Loss: 4.5668
[12/150][654/1083] Loss: 4.5664
[12/150][763/1083] Loss: 4.5212
[12/150][872/1083] Loss: 4.5660
[12/150][981/1083] Loss: 4.5667
Test set: Average loss: 0.1420, Accuracy: 2095/17367 (12%)
[13/150][0/1083] Loss: 4.5035
[13/150][109/1083] Loss: 4.5258
[13/150][218/1083] Loss: 4.6289
[13/150][327/1083] Loss: 4.5668
[13/150][436/1083] Loss: 4.4420
[13/150][545/1083] Loss: 4.5252
[13/150][654/1083] Loss: 4.5668
[13/150][763/1083] Loss: 4.5087
[13/150][872/1083] Loss: 4.4633
[13/150][981/1083] Loss: 4.5353
Test set: Average loss: 0.1418, Accuracy: 2167/17367 (12%)
[14/150][0/1083] Loss: 4.5041
[14/150][109/1083] Loss: 4.5666
[14/150][218/1083] Loss: 4.5661
[14/150][327/1083] Loss: 4.3793
[14/150][436/1083] Loss: 4.4425
[14/150][545/1083] Loss: 4.3791
[14/150][654/1083] Loss: 4.5361
[14/150][763/1083] Loss: 4.3480
[14/150][872/1083] Loss: 4.5768
[14/150][981/1083] Loss: 4.6037
Test set: Average loss: 0.1418, Accuracy: 2165/17367 (12%)
[15/150][0/1083] Loss: 4.4972
[15/150][109/1083] Loss: 4.5350
[15/150][218/1083] Loss: 4.4732
[15/150][327/1083] Loss: 4.4724
[15/150][436/1083] Loss: 4.5420
[15/150][545/1083] Loss: 4.5679
[15/150][654/1083] Loss: 4.4731
[15/150][763/1083] Loss: 4.3643
[15/150][872/1083] Loss: 4.5355
[15/150][981/1083] Loss: 4.5442
Test set: Average loss: 0.1417, Accuracy: 2240/17367 (12%)
[16/150][0/1083] Loss: 4.5600
[16/150][109/1083] Loss: 4.5473
[16/150][218/1083] Loss: 4.4148
[16/150][327/1083] Loss: 4.4727
[16/150][436/1083] Loss: 4.5354
[16/150][545/1083] Loss: 4.5452
[16/150][654/1083] Loss: 4.5649
[16/150][763/1083] Loss: 4.5665
[16/150][872/1083] Loss: 4.5666
[16/150][981/1083] Loss: 4.6097
Test set: Average loss: 0.1416, Accuracy: 2309/17367 (13%)
[17/150][0/1083] Loss: 4.5649
[17/150][109/1083] Loss: 4.5669
[17/150][218/1083] Loss: 4.5681
[17/150][327/1083] Loss: 4.6606
[17/150][436/1083] Loss: 4.4728
[17/150][545/1083] Loss: 4.4929
[17/150][654/1083] Loss: 4.6284
[17/150][763/1083] Loss: 4.5349
[17/150][872/1083] Loss: 4.6288
[17/150][981/1083] Loss: 4.5356
Test set: Average loss: 0.1415, Accuracy: 2323/17367 (13%)
[18/150][0/1083] Loss: 4.5059
[18/150][109/1083] Loss: 4.5528
[18/150][218/1083] Loss: 4.5045
[18/150][327/1083] Loss: 4.6251
[18/150][436/1083] Loss: 4.6016
[18/150][545/1083] Loss: 4.4990
[18/150][654/1083] Loss: 4.5741
[18/150][763/1083] Loss: 4.5039
[18/150][872/1083] Loss: 4.5354
[18/150][981/1083] Loss: 4.5510
Test set: Average loss: 0.1419, Accuracy: 2124/17367 (12%)
[19/150][0/1083] Loss: 4.5665
[19/150][109/1083] Loss: 4.5713
[19/150][218/1083] Loss: 4.5354
[19/150][327/1083] Loss: 4.5979
[19/150][436/1083] Loss: 4.4732
[19/150][545/1083] Loss: 4.5228
[19/150][654/1083] Loss: 4.5351
[19/150][763/1083] Loss: 4.4800
[19/150][872/1083] Loss: 4.5977
[19/150][981/1083] Loss: 4.5125
Test set: Average loss: 0.1417, Accuracy: 2243/17367 (12%)
[20/150][0/1083] Loss: 4.3899
[20/150][109/1083] Loss: 4.4120
[20/150][218/1083] Loss: 4.6289
[20/150][327/1083] Loss: 4.5963
[20/150][436/1083] Loss: 4.5120
[20/150][545/1083] Loss: 4.5902
[20/150][654/1083] Loss: 4.5981
[20/150][763/1083] Loss: 4.5631
[20/150][872/1083] Loss: 4.5980
[20/150][981/1083] Loss: 4.4731
Test set: Average loss: 0.1416, Accuracy: 2313/17367 (13%)
[21/150][0/1083] Loss: 4.5354
[21/150][109/1083] Loss: 4.5422
[21/150][218/1083] Loss: 4.5046
[21/150][327/1083] Loss: 4.5355
[21/150][436/1083] Loss: 4.5046
[21/150][545/1083] Loss: 4.4743
[21/150][654/1083] Loss: 4.5665
[21/150][763/1083] Loss: 4.5024
[21/150][872/1083] Loss: 4.5615
[21/150][981/1083] Loss: 4.4734
Test set: Average loss: 0.1416, Accuracy: 2320/17367 (13%)
[22/150][0/1083] Loss: 4.5043
[22/150][109/1083] Loss: 4.4730
[22/150][218/1083] Loss: 4.5950
[22/150][327/1083] Loss: 4.4732
[22/150][436/1083] Loss: 4.5370
[22/150][545/1083] Loss: 4.4417
[22/150][654/1083] Loss: 4.5980
[22/150][763/1083] Loss: 4.5152
[22/150][872/1083] Loss: 4.5044
[22/150][981/1083] Loss: 4.5145
Test set: Average loss: 0.1416, Accuracy: 2278/17367 (13%)
[23/150][0/1083] Loss: 4.5669
[23/150][109/1083] Loss: 4.5355
[23/150][218/1083] Loss: 4.5249
[23/150][327/1083] Loss: 4.5357
[23/150][436/1083] Loss: 4.5982
[23/150][545/1083] Loss: 4.5421
[23/150][654/1083] Loss: 4.5669
[23/150][763/1083] Loss: 4.4703
[23/150][872/1083] Loss: 4.5042
[23/150][981/1083] Loss: 4.5666
Test set: Average loss: 0.1417, Accuracy: 2216/17367 (12%)
[24/150][0/1083] Loss: 4.5043
[24/150][109/1083] Loss: 4.5650
[24/150][218/1083] Loss: 4.5352
[24/150][327/1083] Loss: 4.5354
[24/150][436/1083] Loss: 4.6263
[24/150][545/1083] Loss: 4.5981
[24/150][654/1083] Loss: 4.5354
[24/150][763/1083] Loss: 4.4621
[24/150][872/1083] Loss: 4.5981
[24/150][981/1083] Loss: 4.5381
Test set: Average loss: 0.1416, Accuracy: 2269/17367 (13%)
[25/150][0/1083] Loss: 4.6293
[25/150][109/1083] Loss: 4.5357
[25/150][218/1083] Loss: 4.6288
[25/150][327/1083] Loss: 4.4418
[25/150][436/1083] Loss: 4.5322
[25/150][545/1083] Loss: 4.4735
[25/150][654/1083] Loss: 4.5669
[25/150][763/1083] Loss: 4.5043
[25/150][872/1083] Loss: 4.5741
[25/150][981/1083] Loss: 4.4733
Test set: Average loss: 0.1417, Accuracy: 2251/17367 (12%)
[26/150][0/1083] Loss: 4.5133
[26/150][109/1083] Loss: 4.5982
[26/150][218/1083] Loss: 4.5101
[26/150][327/1083] Loss: 4.5668
[26/150][436/1083] Loss: 4.5046
[26/150][545/1083] Loss: 4.3566
[26/150][654/1083] Loss: 4.5158
[26/150][763/1083] Loss: 4.4685
[26/150][872/1083] Loss: 4.6294
[26/150][981/1083] Loss: 4.6607
Test set: Average loss: 0.1415, Accuracy: 2327/17367 (13%)
[27/150][0/1083] Loss: 4.5353
[27/150][109/1083] Loss: 4.5361
[27/150][218/1083] Loss: 4.4732
[27/150][327/1083] Loss: 4.5041
[27/150][436/1083] Loss: 4.6289
[27/150][545/1083] Loss: 4.5980
[27/150][654/1083] Loss: 4.4714
[27/150][763/1083] Loss: 4.5980
[27/150][872/1083] Loss: 4.5040
[27/150][981/1083] Loss: 4.6294
Test set: Average loss: 0.1416, Accuracy: 2295/17367 (13%)
[28/150][0/1083] Loss: 4.5976
[28/150][109/1083] Loss: 4.4103
[28/150][218/1083] Loss: 4.4652
[28/150][327/1083] Loss: 4.5655
[28/150][436/1083] Loss: 4.5984
[28/150][545/1083] Loss: 4.5087
[28/150][654/1083] Loss: 4.4131
[28/150][763/1083] Loss: 4.4622
[28/150][872/1083] Loss: 4.5660
[28/150][981/1083] Loss: 4.5433
Test set: Average loss: 0.1416, Accuracy: 2287/17367 (13%)
[29/150][0/1083] Loss: 4.5981
[29/150][109/1083] Loss: 4.5665
[29/150][218/1083] Loss: 4.5668
[29/150][327/1083] Loss: 4.5676
[29/150][436/1083] Loss: 4.5741
[29/150][545/1083] Loss: 4.5045
[29/150][654/1083] Loss: 4.5358
[29/150][763/1083] Loss: 4.6280
[29/150][872/1083] Loss: 4.5980
[29/150][981/1083] Loss: 4.5356
Test set: Average loss: 0.1417, Accuracy: 2250/17367 (12%)
[30/150][0/1083] Loss: 4.5014
[30/150][109/1083] Loss: 4.4107
[30/150][218/1083] Loss: 4.5358
[30/150][327/1083] Loss: 4.5044
[30/150][436/1083] Loss: 4.4421
[30/150][545/1083] Loss: 4.5482
[30/150][654/1083] Loss: 4.4714
[30/150][763/1083] Loss: 4.5044
[30/150][872/1083] Loss: 4.4148
[30/150][981/1083] Loss: 4.5666
Test set: Average loss: 0.1417, Accuracy: 2207/17367 (12%)
[31/150][0/1083] Loss: 4.5402
[31/150][109/1083] Loss: 4.5043
[31/150][218/1083] Loss: 4.4419
[31/150][327/1083] Loss: 4.5845
[31/150][436/1083] Loss: 4.4424
[31/150][545/1083] Loss: 4.5667
[31/150][654/1083] Loss: 4.6294
[31/150][763/1083] Loss: 4.5664
[31/150][872/1083] Loss: 4.4733
[31/150][981/1083] Loss: 4.5669
Test set: Average loss: 0.1417, Accuracy: 2213/17367 (12%)
[32/150][0/1083] Loss: 4.5370
[32/150][109/1083] Loss: 4.4417
[32/150][218/1083] Loss: 4.5351
[32/150][327/1083] Loss: 4.5354
[32/150][436/1083] Loss: 4.5356
[32/150][545/1083] Loss: 4.4424
[32/150][654/1083] Loss: 4.5067
[32/150][763/1083] Loss: 4.5046
[32/150][872/1083] Loss: 4.4729
[32/150][981/1083] Loss: 4.5776
Test set: Average loss: 0.1417, Accuracy: 2223/17367 (12%)
[33/150][0/1083] Loss: 4.5609
[33/150][109/1083] Loss: 4.5667
[33/150][218/1083] Loss: 4.5362
[33/150][327/1083] Loss: 4.5354
[33/150][436/1083] Loss: 4.3291
[33/150][545/1083] Loss: 4.5993
[33/150][654/1083] Loss: 4.5983
[33/150][763/1083] Loss: 4.4948
[33/150][872/1083] Loss: 4.5979
[33/150][981/1083] Loss: 4.6603
Test set: Average loss: 0.1416, Accuracy: 2318/17367 (13%)
[34/150][0/1083] Loss: 4.6141
[34/150][109/1083] Loss: 4.5667
[34/150][218/1083] Loss: 4.4419
[34/150][327/1083] Loss: 4.5347
[34/150][436/1083] Loss: 4.5669
[34/150][545/1083] Loss: 4.5355
[34/150][654/1083] Loss: 4.5982
[34/150][763/1083] Loss: 4.5215
[34/150][872/1083] Loss: 4.4743
[34/150][981/1083] Loss: 4.4927
Test set: Average loss: 0.1416, Accuracy: 2286/17367 (13%)
[35/150][0/1083] Loss: 4.4733
[35/150][109/1083] Loss: 4.4418
[35/150][218/1083] Loss: 4.5669
[35/150][327/1083] Loss: 4.5360
[35/150][436/1083] Loss: 4.5357
[35/150][545/1083] Loss: 4.4731
[35/150][654/1083] Loss: 4.5978
[35/150][763/1083] Loss: 4.5044
[35/150][872/1083] Loss: 4.5986
[35/150][981/1083] Loss: 4.5983
Test set: Average loss: 0.1415, Accuracy: 2347/17367 (13%)
[36/150][0/1083] Loss: 4.4706
[36/150][109/1083] Loss: 4.4730
[36/150][218/1083] Loss: 4.5045
[36/150][327/1083] Loss: 4.5360
[36/150][436/1083] Loss: 4.4738
[36/150][545/1083] Loss: 4.5205
[36/150][654/1083] Loss: 4.5361
[36/150][763/1083] Loss: 4.6604
[36/150][872/1083] Loss: 4.4732
[36/150][981/1083] Loss: 4.5980
Test set: Average loss: 0.1417, Accuracy: 2243/17367 (12%)
[37/150][0/1083] Loss: 4.5664
[37/150][109/1083] Loss: 4.5217
[37/150][218/1083] Loss: 4.5667
[37/150][327/1083] Loss: 4.5045
[37/150][436/1083] Loss: 4.5668
[37/150][545/1083] Loss: 4.5044
[37/150][654/1083] Loss: 4.5039
[37/150][763/1083] Loss: 4.4421
[37/150][872/1083] Loss: 4.5982
[37/150][981/1083] Loss: 4.4548
Test set: Average loss: 0.1415, Accuracy: 2372/17367 (13%)
[38/150][0/1083] Loss: 4.4103
[38/150][109/1083] Loss: 4.4728
[38/150][218/1083] Loss: 4.5670
[38/150][327/1083] Loss: 4.4630
[38/150][436/1083] Loss: 4.5356
[38/150][545/1083] Loss: 4.4729
[38/150][654/1083] Loss: 4.5671
[38/150][763/1083] Loss: 4.4667
[38/150][872/1083] Loss: 4.5433
[38/150][981/1083] Loss: 4.4107
Test set: Average loss: 0.1415, Accuracy: 2329/17367 (13%)
[39/150][0/1083] Loss: 4.5354
[39/150][109/1083] Loss: 4.5358
[39/150][218/1083] Loss: 4.5231
[39/150][327/1083] Loss: 4.5129
[39/150][436/1083] Loss: 4.5980
[39/150][545/1083] Loss: 4.6295
[39/150][654/1083] Loss: 4.4526
[39/150][763/1083] Loss: 4.5043
[39/150][872/1083] Loss: 4.5977
[39/150][981/1083] Loss: 4.4517
Test set: Average loss: 0.1416, Accuracy: 2291/17367 (13%)
[40/150][0/1083] Loss: 4.5663
[40/150][109/1083] Loss: 4.4114
[40/150][218/1083] Loss: 4.4737
[40/150][327/1083] Loss: 4.5043
[40/150][436/1083] Loss: 4.4412
[40/150][545/1083] Loss: 4.5985
[40/150][654/1083] Loss: 4.3802
[40/150][763/1083] Loss: 4.4736
[40/150][872/1083] Loss: 4.4420
[40/150][981/1083] Loss: 4.5138
Test set: Average loss: 0.1417, Accuracy: 2229/17367 (12%)
[41/150][0/1083] Loss: 4.5042
[41/150][109/1083] Loss: 4.5045
[41/150][218/1083] Loss: 4.4825
[41/150][327/1083] Loss: 4.4795
[41/150][436/1083] Loss: 4.4991
[41/150][545/1083] Loss: 4.5354
[41/150][654/1083] Loss: 4.5982
[41/150][763/1083] Loss: 4.5039
[41/150][872/1083] Loss: 4.5967
[41/150][981/1083] Loss: 4.5547
Test set: Average loss: 0.1418, Accuracy: 2205/17367 (12%)
[42/150][0/1083] Loss: 4.6264
[42/150][109/1083] Loss: 4.5160
[42/150][218/1083] Loss: 4.4420
[42/150][327/1083] Loss: 4.4729
[42/150][436/1083] Loss: 4.5982
[42/150][545/1083] Loss: 4.5547
[42/150][654/1083] Loss: 4.5053
[42/150][763/1083] Loss: 4.5678
[42/150][872/1083] Loss: 4.5041
[42/150][981/1083] Loss: 4.5592
Test set: Average loss: 0.1416, Accuracy: 2273/17367 (13%)
[43/150][0/1083] Loss: 4.5047
[43/150][109/1083] Loss: 4.5043
[43/150][218/1083] Loss: 4.4416
[43/150][327/1083] Loss: 4.5665
[43/150][436/1083] Loss: 4.4419
[43/150][545/1083] Loss: 4.5624
[43/150][654/1083] Loss: 4.5981
[43/150][763/1083] Loss: 4.5353
[43/150][872/1083] Loss: 4.5617
[43/150][981/1083] Loss: 4.4421
Test set: Average loss: 0.1416, Accuracy: 2302/17367 (13%)
[44/150][0/1083] Loss: 4.5356
[44/150][109/1083] Loss: 4.4486
[44/150][218/1083] Loss: 4.6293
[44/150][327/1083] Loss: 4.5355
[44/150][436/1083] Loss: 4.5669
[44/150][545/1083] Loss: 4.5669
[44/150][654/1083] Loss: 4.5352
[44/150][763/1083] Loss: 4.4112
[44/150][872/1083] Loss: 4.5647
[44/150][981/1083] Loss: 4.5384
Test set: Average loss: 0.1416, Accuracy: 2324/17367 (13%)
[45/150][0/1083] Loss: 4.5357
[45/150][109/1083] Loss: 4.4735
[45/150][218/1083] Loss: 4.4420
[45/150][327/1083] Loss: 4.5358
[45/150][436/1083] Loss: 4.5357
[45/150][545/1083] Loss: 4.4642
[45/150][654/1083] Loss: 4.5044
[45/150][763/1083] Loss: 4.5357
[45/150][872/1083] Loss: 4.4734
[45/150][981/1083] Loss: 4.4417
Test set: Average loss: 0.1417, Accuracy: 2201/17367 (12%)
[46/150][0/1083] Loss: 4.5981
[46/150][109/1083] Loss: 4.4106
[46/150][218/1083] Loss: 4.4731
[46/150][327/1083] Loss: 4.5044
[46/150][436/1083] Loss: 4.5668
[46/150][545/1083] Loss: 4.4113
[46/150][654/1083] Loss: 4.4424
[46/150][763/1083] Loss: 4.4730
[46/150][872/1083] Loss: 4.5357
^CProcess Process-186:
Process Process-185:
Traceback (most recent call last):
  File "main_ggnn.py", line 102, in <module>
    main(opt)
  File "main_ggnn.py", line 93, in main
    train(epoch, train_dataloader, net, criterion, optimizer, opt, writer)
  File "/e/utils/train_ggnn.py", line 27, in train
    writer.add_graph(net, (init_input, adj_matrix), verbose=False)
  File "/opt/conda/lib/python3.6/site-packages/tensorboardX/writer.py", line 520, in add_graph
    self.file_writer.add_graph(graph(model, input_to_model, verbose))
  File "/opt/conda/lib/python3.6/site-packages/tensorboardX/pytorch_graph.py", line 98, in graph
    torch.onnx._optimize_trace(trace, False)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py", line 30, in _optimize_trace
    trace.set_graph(utils._optimize_graph(trace.graph(), aten))
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 95, in _optimize_graph
    graph = torch._C._jit_pass_onnx(graph, aten)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/__init__.py", line 40, in _run_symbolic_function
    return utils._run_symbolic_function(*args, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/utils.py", line 368, in _run_symbolic_function
    return fn(g, *inputs, **attrs)
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic.py", line 150, in mul
    return g.op("Mul", self, _if_scalar_type_as(other, self), **_broadcast_if_scalar(other))
  File "/opt/conda/lib/python3.6/site-packages/torch/onnx/symbolic.py", line 49, in _if_scalar_type_as
    if isinstance(self, torch._C.Value):
KeyboardInterrupt
(10%)
(11%)
(11%)
(11%)
(11%)
(11%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(12%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
(13%)
